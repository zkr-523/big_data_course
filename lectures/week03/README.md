# Week 3: MapReduce Fundamentals

## Overview

This week introduces the MapReduce programming model - the foundational paradigm for distributed data processing. Students will learn how to break down complex data analysis tasks into Map and Reduce phases that can run across a cluster.

---

## Session 3A: MapReduce Concepts

### Learning Objectives
1. Understand the MapReduce programming model (Map â†’ Shuffle â†’ Reduce)
2. Explain how MapReduce achieves parallel processing
3. Identify when MapReduce is appropriate vs. other approaches
4. Trace data flow through a MapReduce job

### Pre-Class Video
**"MapReduce Explained"** - Computerphile (~15 min)  
ğŸ”— https://www.youtube.com/watch?v=cvhKoniK5Uo

**Alternative**: "MapReduce Basics" - Simplilearn (~20 min)  
ğŸ”— https://www.youtube.com/watch?v=SqvAaB3vK8U

### Materials
- ğŸ“Š Slides: `slides/SE446_W03A_mapreduce_concepts.pdf`
- ğŸ““ Notebook: `notebooks/SE446_W03A_mapreduce_intro.ipynb`

---

## Session 3B: Implementing MapReduce in Python

### Learning Objectives
1. Implement Mapper functions in Python
2. Implement Reducer functions in Python
3. Chain Map and Reduce operations for multi-stage processing
4. Debug common MapReduce errors

### Pre-Class Video
**"Python MapReduce Tutorial"** - Derek Banas (~18 min)  
ğŸ”— https://www.youtube.com/watch?v=nOKVh3t6x4g

### Materials
- ğŸ“Š Slides: `slides/SE446_W03B_mapreduce_python.pdf`
- ğŸ““ Notebook: `notebooks/SE446_W03B_mapreduce_practice.ipynb`
- ğŸ”¬ Lab: `labs/02_mapreduce_lab/`

---

## Key Concepts

### MapReduce Paradigm

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          MAPREDUCE DATA FLOW                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  INPUT DATA                                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                              â”‚
â”‚  Block 1: "Hello World Hello"                                              â”‚
â”‚  Block 2: "World Big Data"                                                 â”‚
â”‚  Block 3: "Hello Big Data World"                                           â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                        MAP PHASE                                    â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚   â”‚
â”‚  â”‚  Mapper 1: "Hello World Hello"                                      â”‚   â”‚
â”‚  â”‚            â†’ (Hello, 1), (World, 1), (Hello, 1)                    â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  Mapper 2: "World Big Data"                                        â”‚   â”‚
â”‚  â”‚            â†’ (World, 1), (Big, 1), (Data, 1)                       â”‚   â”‚
â”‚  â”‚                                                                     â”‚   â”‚
â”‚  â”‚  Mapper 3: "Hello Big Data World"                                  â”‚   â”‚
â”‚  â”‚            â†’ (Hello, 1), (Big, 1), (Data, 1), (World, 1)           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                     SHUFFLE & SORT                                  â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚   â”‚
â”‚  â”‚  Group by key:                                                      â”‚   â”‚
â”‚  â”‚    Big   â†’ [1, 1]                                                  â”‚   â”‚
â”‚  â”‚    Data  â†’ [1, 1]                                                  â”‚   â”‚
â”‚  â”‚    Hello â†’ [1, 1, 1]                                               â”‚   â”‚
â”‚  â”‚    World â†’ [1, 1, 1]                                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                       REDUCE PHASE                                  â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚   â”‚
â”‚  â”‚  Reducer: Sum values for each key                                  â”‚   â”‚
â”‚  â”‚    Big   â†’ 2                                                       â”‚   â”‚
â”‚  â”‚    Data  â†’ 2                                                       â”‚   â”‚
â”‚  â”‚    Hello â†’ 3                                                       â”‚   â”‚
â”‚  â”‚    World â†’ 3                                                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                                                 â”‚
â”‚           â–¼                                                                 â”‚
â”‚  OUTPUT: {Big: 2, Data: 2, Hello: 3, World: 3}                             â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Terminology

| Term | Definition |
|------|------------|
| **Mapper** | Function that processes input records and emits (key, value) pairs |
| **Reducer** | Function that aggregates values for each unique key |
| **Shuffle** | System process that groups mapper outputs by key |
| **Combiner** | Optional local reducer that runs on mapper output (optimization) |
| **Partitioner** | Determines which reducer receives which keys |

### MapReduce vs Traditional Processing

| Aspect | Traditional | MapReduce |
|--------|-------------|-----------|
| **Data Location** | Move data to computation | Move computation to data |
| **Parallelism** | Manual threading | Automatic across cluster |
| **Fault Tolerance** | Custom handling | Built-in re-execution |
| **Scalability** | Limited by single machine | Linear with cluster size |

---

## In-Class Lab: Crime Analysis with MapReduce

Students will implement MapReduce to analyze Chicago crime data:

### Lab Tasks
1. **Word Count** - Count crime types
2. **Aggregation** - Sum crimes per district
3. **Filtering** - Find crimes with arrests
4. **Multi-stage** - Top 5 crime locations

### Sample Code Pattern

```python
# Mapper: emit (crime_type, 1) for each crime
def crime_type_mapper(record):
    crime_type = record['Primary Type']
    return (crime_type, 1)

# Reducer: sum all counts for each crime type
def count_reducer(key, values):
    return (key, sum(values))
```

---

## ExamGPT Topics

The in-class quiz will cover:
- Map function input/output format
- Reduce function input/output format
- Shuffle phase purpose
- Identifying parallelization opportunities
- Debugging MapReduce logic errors

---

## Homework

Before Week 4:
1. Complete Milestone 2 starter notebook (MapReduce)
2. Implement mapper and reducer for crime analysis
3. Commit your work to GitHub with meaningful messages
4. Watch pre-class videos for Week 4

---

## Connection to HDFS (Week 2)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   HOW MAPREDUCE USES HDFS                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. INPUT: Data stored in HDFS blocks across DataNodes                     â”‚
â”‚                                                                             â”‚
â”‚  2. DATA LOCALITY: Mappers run ON the DataNode containing the block        â”‚
â”‚     â†’ Minimizes network transfer (computation moves to data)               â”‚
â”‚                                                                             â”‚
â”‚  3. OUTPUT: Reducer results written back to HDFS                           â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚  DataNode 1  â”‚     â”‚  DataNode 2  â”‚     â”‚  DataNode 3  â”‚                â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚     â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚     â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚                â”‚
â”‚  â”‚  Block A     â”‚     â”‚  Block B     â”‚     â”‚  Block C     â”‚                â”‚
â”‚  â”‚      â†“       â”‚     â”‚      â†“       â”‚     â”‚      â†“       â”‚                â”‚
â”‚  â”‚  Mapper 1    â”‚     â”‚  Mapper 2    â”‚     â”‚  Mapper 3    â”‚                â”‚
â”‚  â”‚  (local)     â”‚     â”‚  (local)     â”‚     â”‚  (local)     â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚          â”‚                   â”‚                   â”‚                          â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                        â–¼                                                    â”‚
â”‚                 [ Shuffle & Sort ]                                         â”‚
â”‚                        â†“                                                    â”‚
â”‚                  [ Reducers ]                                              â”‚
â”‚                        â†“                                                    â”‚
â”‚                   HDFS Output                                              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Resources

- ğŸ“– **Textbook Chapter 3**: MapReduce Fundamentals
- ğŸ”— **Apache Hadoop Docs**: https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/
- ğŸ“º **Google's Original Paper**: "MapReduce: Simplified Data Processing on Large Clusters"
