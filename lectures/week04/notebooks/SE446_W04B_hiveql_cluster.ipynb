{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa1b2c3d",
   "metadata": {},
   "source": [
    "# SE446 - Week 4B: HiveQL on the Hadoop Cluster\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Connect to HiveServer2 using Beeline\n",
    "2. Understand the Hive Metastore (before/after table creation)\n",
    "3. Create external Hive tables over HDFS data\n",
    "4. Run analytical HiveQL queries on real crime data\n",
    "5. Create and execute `.hql` script files (professional practice)\n",
    "6. Compare the HiveQL approach with MapReduce (Milestone 1)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2c3d4e",
   "metadata": {},
   "source": [
    "## âš ï¸ Important: This Notebook Uses Terminal Commands\n",
    "\n",
    "Unlike Notebook 4A (which ran everything in Python), this notebook guides you through running **real HiveQL commands on the Hadoop cluster** via your terminal.\n",
    "\n",
    "### How to follow along\n",
    "\n",
    "1. **Open a terminal** (separate from this notebook)\n",
    "2. **SSH into the cluster** using the commands below\n",
    "3. **Run each HiveQL command** shown in the code cells\n",
    "4. **Paste your results** into the answer cells provided\n",
    "\n",
    "The code cells below show you the exact commands to type. **Do NOT run them in this notebook** â€” run them in your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d4e5f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Prerequisites âœ…\n",
    "\n",
    "Before starting, make sure you have:\n",
    "\n",
    "- [ ] SSH access to the cluster (same credentials from Milestone 1)\n",
    "- [ ] Completed Notebook 4A (HiveQL Emulator)\n",
    "- [ ] The Chicago crime dataset is already on HDFS at `/data/chicago_crimes.csv`\n",
    "\n",
    "### Dataset Reminder\n",
    "\n",
    "This is the **same Chicago crime dataset** you used in Milestone 1 for MapReduce.\n",
    "\n",
    "| File | Path | Size |\n",
    "|------|------|------|\n",
    "| Full dataset | `/data/chicago_crimes.csv` | 8M+ rows |\n",
    "| Sample | `/data/chicago_crimes_sample.csv` | 10K rows |\n",
    "\n",
    "**Schema**: `ID, Case Number, Date, Block, IUCR, Primary Type, Description, Location Description, Arrest, Domestic, ...`\n",
    "\n",
    "| Index | Column | Example |\n",
    "|-------|--------|---------|\n",
    "| 0 | ID | 10224738 |\n",
    "| 1 | Case Number | HY411648 |\n",
    "| 2 | Date | 09/05/2015 01:30:00 PM |\n",
    "| 3 | Block | 043XX S WOOD ST |\n",
    "| 4 | IUCR | 0820 |\n",
    "| 5 | Primary Type | THEFT |\n",
    "| 6 | Description | $500 AND UNDER |\n",
    "| 7 | Location Description | STREET |\n",
    "| 8 | Arrest | false |\n",
    "| 9 | Domestic | false |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4e5f6a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Step 1: Connect to the Cluster ðŸ–¥ï¸\n",
    "\n",
    "Open your terminal and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f6a7b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  RUN THESE COMMANDS IN YOUR TERMINAL (not in this notebook) â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Step 1: SSH into the cluster\n",
    "ssh your_id@134.209.172.50\n",
    "\n",
    "# Step 2: Source Hadoop environment (REQUIRED every login)\n",
    "source /etc/profile.d/hadoop.sh\n",
    "\n",
    "# Step 3: Verify the dataset exists on HDFS\n",
    "hdfs dfs -ls /data/chicago_crimes*.csv\n",
    "\n",
    "# Step 4: Check the first 3 lines (to see the header + data)\n",
    "hdfs dfs -cat /data/chicago_crimes_sample.csv | head -3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019b1b4",
   "metadata": {},
   "source": [
    "### ðŸ“ Paste your output here\n",
    "\n",
    "After running the `head -3` command above, paste the first 3 lines of the CSV below.\n",
    "\n",
    "**Your output:**\n",
    "\n",
    "```\n",
    "(paste here)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b8c9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Step 2: Connect to Hive with Beeline ðŸ\n",
    "\n",
    "### `hive` vs `beeline -u 'jdbc:hive2://'` â€” What's the Difference?\n",
    "\n",
    "Both commands start Beeline, but they behave differently:\n",
    "\n",
    "| | `hive` | `beeline -u 'jdbc:hive2://'` |\n",
    "|---|--------|-------------------------------|\n",
    "| **What it does** | Starts Beeline but does NOT connect | Starts Beeline AND connects immediately |\n",
    "| **Prompt** | `beeline>` (not connected) | `0: jdbc:hive2://>` (connected) |\n",
    "| **Run queries?** | âŒ No â€” \"No current connection\" error | âœ… Yes â€” ready immediately |\n",
    "| **Use case** | Only if you want to connect manually after | Recommended â€” connect and work in one step |\n",
    "\n",
    "> âš ï¸ **Common Mistake**: Students type `hive`, see the `beeline>` prompt, and try to run queries. This gives \"No current connection\" errors. **Always use `beeline -u 'jdbc:hive2://'` to connect properly.**\n",
    "\n",
    "### How to Connect\n",
    "\n",
    "```bash\n",
    "# RECOMMENDED: Connect directly (one command, ready to query)\n",
    "beeline -u 'jdbc:hive2://'\n",
    "\n",
    "# NOT RECOMMENDED: This starts Beeline but does NOT connect\n",
    "# hive\n",
    "# (you would then need to type: !connect jdbc:hive2://)\n",
    "```\n",
    "\n",
    "You will see this prompt when connected:\n",
    "```\n",
    "Connected to: Apache Hive (version 4.0.1)\n",
    "0: jdbc:hive2://>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c9d0e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  RUN THIS IN YOUR TERMINAL                                   â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# CORRECT: Connect to Hive (embedded mode - works for all users)\n",
    "beeline -u 'jdbc:hive2://'\n",
    "\n",
    "# Expected output:\n",
    "# Connected to: Apache Hive (version 4.0.1)\n",
    "# Driver: Hive JDBC (version 4.0.1)\n",
    "# 0: jdbc:hive2://>\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# WRONG: This starts Beeline but does NOT connect!\n",
    "# hive\n",
    "# (gives \"beeline>\" prompt but queries will fail with \"No current connection\")\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# To exit Beeline when done:\n",
    "# !quit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9d0e1f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Understanding the Hive Metastore ðŸ“¦\n",
    "\n",
    "### What is the Metastore?\n",
    "\n",
    "The **Hive Metastore** stores metadata about your tables:\n",
    "- Table names, column names, data types\n",
    "- Where the data files are located (HDFS path)\n",
    "- Table properties (delimiters, file format, etc.)\n",
    "\n",
    "> **Key Concept**: The metastore does NOT store the actual data â€” it only stores information ABOUT the data. The data stays on HDFS.\n",
    "\n",
    "### Let's See the Metastore BEFORE Creating a Table\n",
    "\n",
    "First, let's check what databases and tables exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e1f2a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  RUN THESE IN BEELINE (after connecting)                     â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Check existing databases\n",
    "SHOW DATABASES;\n",
    "\n",
    "# Create your own database (use your student ID)\n",
    "CREATE DATABASE IF NOT EXISTS your_id_db;\n",
    "USE your_id_db;\n",
    "\n",
    "# Check tables BEFORE creating anything\n",
    "SHOW TABLES;\n",
    "# Expected output: empty (no tables yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f2a3b",
   "metadata": {},
   "source": [
    "### ðŸ“ Metastore BEFORE Creating Table\n",
    "\n",
    "**`SHOW DATABASES;` output:**\n",
    "```\n",
    "default\n",
    "your_id_db\n",
    "```\n",
    "\n",
    "**`SHOW TABLES;` output (in your_id_db):**\n",
    "```\n",
    "(empty - no tables yet)\n",
    "```\n",
    "\n",
    "> **ðŸ’¡ Observation**: The metastore knows about databases but has no table metadata yet. Let's create a table and see what changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a3b4c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Step 3: Create an External Table ðŸ—ï¸\n",
    "\n",
    "### Method 1: Paste the Full Query Directly\n",
    "\n",
    "You can copy and paste the **entire** CREATE TABLE statement at once. Hive will wait for the semicolon (`;`) before executing.\n",
    "\n",
    "> âš ï¸ **Do NOT copy line by line** â€” paste the whole statement at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3b4c5d",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "-- â•‘  COPY THIS ENTIRE BLOCK AND PASTE IN BEELINE                â•‘\n",
    "-- â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "CREATE EXTERNAL TABLE chicago_crimes (\n",
    "    id                   STRING,\n",
    "    case_number          STRING,\n",
    "    crime_date           STRING,\n",
    "    block                STRING,\n",
    "    iucr                 STRING,\n",
    "    primary_type         STRING,\n",
    "    description          STRING,\n",
    "    location_description STRING,\n",
    "    arrest               STRING,\n",
    "    domestic             STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/data/'\n",
    "TBLPROPERTIES ('skip.header.line.count'='1');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c5d6e",
   "metadata": {},
   "source": [
    "### Metastore AFTER Creating Table\n",
    "\n",
    "Now check the metastore again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5d6e7f",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Check tables AFTER creating\n",
    "SHOW TABLES;\n",
    "\n",
    "-- See the table metadata stored in metastore\n",
    "DESCRIBE chicago_crimes;\n",
    "\n",
    "-- See detailed metadata (storage location, format, etc.)\n",
    "DESCRIBE FORMATTED chicago_crimes;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b6d2e",
   "metadata": {},
   "source": [
    "### ðŸ“ Metastore AFTER Creating Table\n",
    "\n",
    "**`SHOW TABLES;` output:**\n",
    "```\n",
    "chicago_crimes\n",
    "```\n",
    "\n",
    "**`DESCRIBE chicago_crimes;` output:**\n",
    "```\n",
    "col_name              data_type\n",
    "id                    string\n",
    "case_number           string\n",
    "crime_date            string\n",
    "block                 string\n",
    "iucr                  string\n",
    "primary_type          string\n",
    "description           string\n",
    "location_description  string\n",
    "arrest                string\n",
    "domestic              string\n",
    "```\n",
    "\n",
    "> **ðŸ’¡ Key Insight**: The metastore now contains:\n",
    "> - Table name and column definitions\n",
    "> - Data types for each column\n",
    "> - Storage location (`/data/` on HDFS)\n",
    "> - File format (TEXTFILE) and delimiter (`,`)\n",
    "> \n",
    "> **The actual CSV data stays on HDFS** â€” the metastore only stores metadata!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7f8a9b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Method 2: Using HQL Script Files (Professional Practice) ðŸ“„\n",
    "\n",
    "In real data engineering, you don't type queries manually. Instead, you save queries in `.hql` files and execute them. This is:\n",
    "- **Repeatable** â€” run the same script anytime\n",
    "- **Version-controlled** â€” save scripts in Git\n",
    "- **Shareable** â€” team members can use your scripts\n",
    "- **Auditable** â€” you have a record of what ran\n",
    "\n",
    "### Step 1: Create a .hql Script File\n",
    "\n",
    "Exit Beeline first (`!quit`), then create a script file:\n",
    "\n",
    "```bash\n",
    "nano top_crimes.hql\n",
    "```\n",
    "\n",
    "Paste the following contents into `top_crimes.hql`:\n",
    "\n",
    "```sql\n",
    "-- Top 10 Crime Types Query\n",
    "-- Make sure you are using your database\n",
    "USE your_id_db;\n",
    "\n",
    "SELECT primary_type, COUNT(*) AS cnt\n",
    "FROM chicago_crimes\n",
    "GROUP BY primary_type\n",
    "ORDER BY cnt DESC\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "Save and exit nano:\n",
    "- **Save**: `CTRL+O`, press Enter\n",
    "- **Exit**: `CTRL+X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ffad3",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Top 10 Crime Types Query\n",
    "-- Make sure you are using your database\n",
    "USE default;\n",
    "\n",
    "SELECT primary_type, COUNT(*) AS cnt\n",
    "FROM chicago_crimes\n",
    "GROUP BY primary_type\n",
    "ORDER BY cnt DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57327dc5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "### Step 2: Execute the HQL Script\n",
    "\n",
    "There are **3 ways** to run a `.hql` file:\n",
    "\n",
    "```bash\n",
    "# Method A: Run from Linux command line (non-interactive)\n",
    "beeline -u 'jdbc:hive2://' -f top_crimes.hql\n",
    "\n",
    "# Method B: Run from inside Beeline using SOURCE\n",
    "beeline -u 'jdbc:hive2://'\n",
    "# Then at the 0: jdbc:hive2://> prompt:\n",
    "SOURCE top_crimes.hql;\n",
    "\n",
    "# Method C: Run a single query inline\n",
    "beeline -u 'jdbc:hive2://' -e \"SELECT COUNT(*) FROM your_id_db.chicago_crimes;\"\n",
    "```\n",
    "\n",
    "> **Best Practice**: Save all your queries in `.hql` files. Submit scripts in assignments, not screenshots!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa32124d",
   "metadata": {},
   "source": [
    "### Step 2: Execute the HQL Script\n",
    "\n",
    "There are **3 ways** to run a `.hql` file:\n",
    "\n",
    "```bash\n",
    "# Method A: Run from Linux command line (non-interactive)\n",
    "beeline -u jdbc:hive2://localhost:10000 -f top_crimes.hql\n",
    "\n",
    "# Method B: Run from inside Beeline using SOURCE\n",
    "beeline -u jdbc:hive2://localhost:10000\n",
    "# Then at the 0: jdbc:hive2://localhost:10000> prompt:\n",
    "SOURCE top_crimes.hql;\n",
    "\n",
    "# Method C: Run with -e flag for single queries\n",
    "beeline -u jdbc:hive2://localhost:10000 -e \"SELECT COUNT(*) FROM your_id_db.chicago_crimes;\"\n",
    "```\n",
    "\n",
    "> **Best Practice**: Save all your queries in `.hql` files. Submit scripts in assignments, not screenshots!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0c1d2e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Quick Sanity Check âœ…\n",
    "\n",
    "Verify the table is working with a simple query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381ed31",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "#Run in Beeline after connecting\n",
    "SELECT * FROM chicago_crimes LIMIT 5;\n",
    "\n",
    "#Count total rows (launches a MapReduce job!)\n",
    "SELECT COUNT(*) FROM chicago_crimes;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e91076",
   "metadata": {},
   "source": [
    "### ðŸ“ Paste your output here\n",
    "\n",
    "**`SELECT * FROM chicago_crimes LIMIT 5;` output:**\n",
    "\n",
    "```\n",
    "(paste here)\n",
    "```\n",
    "\n",
    "**`SELECT COUNT(*) FROM chicago_crimes;` output:**\n",
    "\n",
    "```\n",
    "Total rows: (paste the number here)\n",
    "```\n",
    "\n",
    "**How long did the COUNT query take?** ______ seconds\n",
    "\n",
    "> **ðŸ’¡ Notice**: Even a simple `COUNT(*)` launches a distributed MapReduce job! You'll see progress bars in your terminal. This is the power of Hive â€” it automatically distributes the work across the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f4a5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Query 1: Crime Type Distribution ðŸ“Š\n",
    "\n",
    "**Question**: What are the top 10 most common crime types in Chicago?\n",
    "\n",
    "This is the same question from **Milestone 1 (Task 2)** where you wrote MapReduce code. Now do it in one SQL query:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4a5b6c",
   "metadata": {},
   "source": [
    "-- Query 1: Top 10 crime types\n",
    "SELECT primary_type, COUNT(*) AS cnt\n",
    "FROM chicago_crimes\n",
    "GROUP BY primary_type\n",
    "ORDER BY cnt DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  RUN THIS IN THE HIVE SHELL (hive>)                          â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Exit the Hive shell\n",
    "# quit;\n",
    "\n",
    "# Exit SSH\n",
    "# exit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6c7d8e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Reflection: MapReduce vs. HiveQL ðŸ¤”\n",
    "\n",
    "You've now solved the **same problems** using two different approaches. Let's compare them.\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| Aspect | MapReduce (Milestone 1) | HiveQL (This Notebook) |\n",
    "|--------|------------------------|------------------------|\n",
    "| **Code needed** | `mapper.py` + `reducer.py` (2 files) | 1 SQL query |\n",
    "| **Lines of code** | ~20-30 lines | ~5-8 lines |\n",
    "| **Skill required** | Python programming | SQL knowledge |\n",
    "| **Flexibility** | Can do any computation | Limited to SQL operations |\n",
    "| **Ease of use** | Harder (manual key-value logic) | Easier (declarative) |\n",
    "| **Under the hood** | You write the MapReduce logic | Hive generates it for you |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d8e9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Summary & Quick Reference\n",
    "\n",
    "### What you learned today\n",
    "\n",
    "1. **Connect to Hive** using `beeline -u 'jdbc:hive2://'`\n",
    "2. **Metastore** stores table metadata (schema, location), not actual data\n",
    "3. **External tables** point to existing HDFS data â€” no data copying\n",
    "4. **One SQL query** replaces mapper + reducer + streaming command\n",
    "5. **HQL files** are the professional way to work with Hive\n",
    "\n",
    "### Quick Reference Commands\n",
    "\n",
    "| Task | Command |\n",
    "|------|---------|\n",
    "| Connect to Hive | `beeline -u 'jdbc:hive2://'` |\n",
    "| Run HQL file | `beeline -u 'jdbc:hive2://' -f script.hql` |\n",
    "| Run single query | `beeline -u 'jdbc:hive2://' -e \"SELECT...\"` |\n",
    "| Exit Beeline | `!quit` |\n",
    "\n",
    "### `hive` vs `beeline -u 'jdbc:hive2://'`\n",
    "\n",
    "| Command | Connects? | Can run queries? |\n",
    "|---------|-----------|-----------------|\n",
    "| `hive` | âŒ No | âŒ \"No current connection\" |\n",
    "| `beeline -u 'jdbc:hive2://'` | âœ… Yes | âœ… Immediately |\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "> **Hive = SQL + Distributed Computing**\n",
    ">\n",
    "> Write SQL â†’ Hive generates MapReduce â†’ Cluster processes data â†’ Results returned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e9f0a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Summary\n",
    "\n",
    "### What you learned today\n",
    "\n",
    "1. **Hive provides SQL on Hadoop** â€” you can query massive HDFS datasets without writing MapReduce code\n",
    "2. **External tables** point to existing HDFS data â€” no data movement needed\n",
    "3. **One SQL query** replaces mapper + reducer + streaming command\n",
    "4. **Under the hood**, Hive still uses MapReduce/Tez â€” you can see the jobs running\n",
    "\n",
    "### Key takeaway\n",
    "\n",
    "> **Hive gives you the productivity of SQL with the power of distributed computing.**\n",
    "> \n",
    "> You don't have to choose â€” Hive translates your SQL into MapReduce/Tez jobs automatically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
