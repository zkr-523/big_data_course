{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ“Š SE446 Lab 01: Dataset Exploration\n",
                "\n",
                "**Course:** SE446 Big Data Analytics  \n",
                "**Week:** 1 - Introduction & Project Setup  \n",
                "**Duration:** ~60 minutes\n",
                "\n",
                "---\n",
                "\n",
                "> ðŸ“– **New to Pandas?** Check out the [Pandas Cheatsheet](#-pandas-cheatsheet) at the end of this notebook!\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this lab, you will be able to:\n",
                "\n",
                "1. **Load** CSV datasets into Pandas DataFrames\n",
                "2. **Explore** dataset schemas, data types, and basic statistics\n",
                "3. **Analyze** patterns in real-world urban data\n",
                "4. **Join** multiple datasets to discover cross-domain insights\n",
                "5. **Understand** why traditional techniques fail at Big Data scale\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ™ï¸ The Smart City Project\n",
                "\n",
                "Throughout this course, you'll build a **Smart City Data Platform** using four real-world datasets:\n",
                "\n",
                "| Dataset | Source | Records (Sample) | Smart City Application |\n",
                "|---------|--------|------------------|------------------------|\n",
                "| **NYC Yellow Taxi** | NYC TLC | 50 | Transportation optimization |\n",
                "| **Chicago Crimes** | Chicago Police | 30 | Public safety analytics |\n",
                "| **NYC Weather** | NOAA | 40 | Environmental monitoring |\n",
                "| **Air Quality Index** | EPA | 40 | Health & pollution tracking |\n",
                "\n",
                "> **ðŸ’¡ Note:** This lab uses **sample datasets** (small files hosted on GitHub) that load instantly. The full Kaggle datasets contain millions of recordsâ€”that's where Big Data tools become essential!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# 1ï¸âƒ£ Setup & Environment\n",
                "\n",
                "First, we'll import the libraries we need. Pandas is Python's most popular library for data manipulation and analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required libraries (only needed in Google Colab)\n",
                "# These are pre-installed in most Python environments\n",
                "!pip install pandas matplotlib seaborn -q\n",
                "\n",
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import time\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Configure display options for better readability\n",
                "pd.set_option('display.max_columns', None)  # Show all columns\n",
                "pd.set_option('display.width', None)        # Don't wrap output\n",
                "\n",
                "print(\"âœ… Libraries imported successfully!\")\n",
                "print(f\"ðŸ“¦ Pandas version: {pd.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Defining Dataset URLs\n",
                "\n",
                "We'll load sample datasets directly from GitHub. These are small CSV files (~50 records each) that match the exact schema of the full Kaggle datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Base URL for sample datasets (hosted on GitHub)\n",
                "BASE_URL = \"https://raw.githubusercontent.com/aniskoubaa/big_data_course/main/data/\"\n",
                "\n",
                "# Dataset URLs\n",
                "TAXI_URL = BASE_URL + \"nyc_taxi_sample.csv\"\n",
                "CRIMES_URL = BASE_URL + \"chicago_crimes_sample.csv\"\n",
                "WEATHER_URL = BASE_URL + \"nyc_weather_sample.csv\"\n",
                "AQI_URL = BASE_URL + \"air_quality_sample.csv\"\n",
                "\n",
                "print(\"ðŸ“ Dataset URLs configured:\")\n",
                "print(f\"   ðŸš• Taxi: {TAXI_URL}\")\n",
                "print(f\"   ðŸš” Crimes: {CRIMES_URL}\")\n",
                "print(f\"   ðŸŒ¤ï¸ Weather: {WEATHER_URL}\")\n",
                "print(f\"   ðŸ’¨ AQI: {AQI_URL}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# 2ï¸âƒ£ NYC Yellow Taxi Dataset\n",
                "\n",
                "## Background\n",
                "\n",
                "The NYC Taxi & Limousine Commission (TLC) publishes trip records for every yellow taxi ride in New York City. This dataset is a **classic Big Data benchmark** used by companies and researchers worldwide.\n",
                "\n",
                "**Real-world scale:** Over **1 billion trips** recorded since 2009!\n",
                "\n",
                "**Smart City Use Cases:**\n",
                "- Optimize taxi availability during rush hours\n",
                "- Predict demand by neighborhood\n",
                "- Analyze tipping behavior patterns\n",
                "- Plan for surge pricing\n",
                "\n",
                "Let's load and explore the sample data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load NYC Taxi dataset from GitHub\n",
                "taxi_df = pd.read_csv(TAXI_URL)\n",
                "\n",
                "print(f\"âœ… Loaded {len(taxi_df)} taxi trip records\")\n",
                "print(f\"ðŸ“Š Number of columns: {len(taxi_df.columns)}\")\n",
                "print(f\"ðŸ’¾ Memory usage: {taxi_df.memory_usage(deep=True).sum() / 1024:.2f} KB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Viewing the Data\n",
                "\n",
                "The `head()` method shows the first 5 rows. This gives us a quick look at the actual data values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View first 5 records\n",
                "taxi_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Understanding the Schema\n",
                "\n",
                "The `info()` method shows us:\n",
                "- Column names\n",
                "- Data types (int, float, object/string)\n",
                "- Non-null counts (helps identify missing data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore schema and data types\n",
                "print(\"ðŸ“‹ Dataset Schema:\")\n",
                "print(\"=\" * 60)\n",
                "taxi_df.info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Statistical Summary\n",
                "\n",
                "The `describe()` method provides:\n",
                "- **count:** Number of non-null values\n",
                "- **mean:** Average value\n",
                "- **std:** Standard deviation (spread of values)\n",
                "- **min/max:** Range of values\n",
                "- **25%/50%/75%:** Quartiles (data distribution)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistical summary of numeric columns\n",
                "taxi_df.describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Checking for Missing Values\n",
                "\n",
                "Real-world data often has missing values. Let's check if our dataset has any."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "missing = taxi_df.isnull().sum()\n",
                "print(\"â“ Missing Values per Column:\")\n",
                "print(missing[missing > 0] if missing.sum() > 0 else \"âœ… No missing values!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Basic Analysis: Fare and Tip Patterns\n",
                "\n",
                "Let's calculate some basic statistics about fares and tips."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate key metrics\n",
                "print(\"ðŸ’° NYC Taxi Trip Analysis:\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"   Average fare:     ${taxi_df['fare_amount'].mean():.2f}\")\n",
                "print(f\"   Average tip:      ${taxi_df['tip_amount'].mean():.2f}\")\n",
                "print(f\"   Average total:    ${taxi_df['total_amount'].mean():.2f}\")\n",
                "print(f\"   Average distance: {taxi_df['trip_distance'].mean():.2f} miles\")\n",
                "print(f\"   Avg passengers:   {taxi_df['passenger_count'].mean():.1f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ“ Expected Conclusions (Taxi Data)\n",
                "\n",
                "From exploring the taxi dataset, you should observe:\n",
                "\n",
                "1. **Average fare is ~$20-25** for a typical NYC trip\n",
                "2. **Tips average around $4-5** (credit card tips only; cash tips aren't recorded)\n",
                "3. **Most trips are short** (2-5 miles) with occasional longer airport runs\n",
                "4. **Payment is mostly credit card** (payment_type = 1)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ‹ï¸ Exercise 1: Calculate Tip Percentage\n",
                "\n",
                "**Task:** Calculate the average tip as a percentage of the fare.\n",
                "\n",
                "**Hint:** `(tip_amount / fare_amount) * 100`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Calculate average tip percentage\n",
                "# Replace 'None' with your code\n",
                "\n",
                "avg_tip_percentage = None  # YOUR CODE HERE\n",
                "\n",
                "# Uncomment to check your answer:\n",
                "# print(f\"Average tip percentage: {avg_tip_percentage:.2f}%\")\n",
                "\n",
                "# Expected: Around 18-22% (typical restaurant tipping in the US)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# 3ï¸âƒ£ Chicago Crimes Dataset\n",
                "\n",
                "## Background\n",
                "\n",
                "The Chicago Police Department publishes incident reports for every crime in the city. This dataset includes the crime type, location (GPS coordinates), whether an arrest was made, and more.\n",
                "\n",
                "**Real-world scale:** Over **7 million records** from 2001 to present!\n",
                "\n",
                "**Smart City Use Cases:**\n",
                "- Predict crime hotspots for patrol allocation\n",
                "- Identify seasonal crime patterns\n",
                "- Analyze arrest rates by crime type\n",
                "- Study neighborhood safety trends"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Chicago Crimes dataset\n",
                "crimes_df = pd.read_csv(CRIMES_URL)\n",
                "\n",
                "print(f\"âœ… Loaded {len(crimes_df)} crime records\")\n",
                "print(f\"ðŸ“Š Number of columns: {len(crimes_df.columns)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View first few records\n",
                "crimes_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Crime Type Distribution\n",
                "\n",
                "Let's see what types of crimes are most common in our sample."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Count crimes by type\n",
                "print(\"ðŸ” Crime Types Distribution:\")\n",
                "print(\"=\" * 40)\n",
                "print(crimes_df['Primary Type'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Arrest Rate Analysis\n",
                "\n",
                "What percentage of crimes result in an arrest?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate arrest rate\n",
                "arrest_rate = crimes_df['Arrest'].mean() * 100\n",
                "domestic_rate = crimes_df['Domestic'].mean() * 100\n",
                "\n",
                "print(\"ðŸ‘® Crime Statistics:\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"   Arrest rate: {arrest_rate:.1f}%\")\n",
                "print(f\"   Domestic-related: {domestic_rate:.1f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ“ Expected Conclusions (Crime Data)\n",
                "\n",
                "From exploring the crime dataset, you should observe:\n",
                "\n",
                "1. **THEFT and BATTERY are most common** crime types\n",
                "2. **Arrest rate is around 30-40%** (many crimes go unsolved)\n",
                "3. **Domestic crimes are ~20-30%** of incidents\n",
                "4. **GPS coordinates** enable geographic hotspot analysis\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ‹ï¸ Exercise 2: Find Most Common Crime Type\n",
                "\n",
                "**Task:** Find the single most common crime type in the dataset.\n",
                "\n",
                "**Hint:** Use `value_counts()` and `idxmax()`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Find the most common crime type\n",
                "# Replace 'None' with your code\n",
                "\n",
                "most_common_crime = None  # YOUR CODE HERE\n",
                "\n",
                "# Uncomment to check your answer:\n",
                "# print(f\"Most common crime type: {most_common_crime}\")\n",
                "\n",
                "# Expected: THEFT or BATTERY (depending on sample)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# 4ï¸âƒ£ NYC Weather Dataset\n",
                "\n",
                "## Background\n",
                "\n",
                "Weather data from NOAA (National Oceanic and Atmospheric Administration) includes daily temperature, precipitation, and conditions for New York City.\n",
                "\n",
                "**Smart City Use Cases:**\n",
                "- Correlate weather with taxi demand\n",
                "- Predict energy consumption based on temperature\n",
                "- Plan for weather-related emergencies\n",
                "\n",
                "In **Milestone 4 (Spark)**, you'll join this weather data with taxi trips to discover patterns like: *\"Do people tip more on rainy days?\"*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load NYC Weather dataset\n",
                "weather_df = pd.read_csv(WEATHER_URL)\n",
                "\n",
                "print(f\"âœ… Loaded {len(weather_df)} weather records\")\n",
                "weather_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Date Parsing & Temperature Analysis\n",
                "\n",
                "The `date` column is loaded as a string. We need to convert it to a proper datetime type for time-based analysis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Parse date column to datetime\n",
                "weather_df['date'] = pd.to_datetime(weather_df['date'])\n",
                "\n",
                "# Temperature statistics\n",
                "print(\"ðŸŒ¡ï¸ NYC Temperature Analysis (January 2024):\")\n",
                "print(\"=\" * 45)\n",
                "print(f\"   Average high: {weather_df['temp_max'].mean():.1f}Â°F\")\n",
                "print(f\"   Average low:  {weather_df['temp_min'].mean():.1f}Â°F\")\n",
                "print(f\"   Hottest day:  {weather_df['temp_max'].max()}Â°F on {weather_df.loc[weather_df['temp_max'].idxmax(), 'date'].strftime('%b %d')}\")\n",
                "print(f\"   Coldest day:  {weather_df['temp_min'].min()}Â°F on {weather_df.loc[weather_df['temp_min'].idxmin(), 'date'].strftime('%b %d')}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Weather conditions distribution\n",
                "print(\"ðŸŒ¤ï¸ Weather Conditions:\")\n",
                "print(\"=\" * 30)\n",
                "print(weather_df['weather_condition'].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ“ Expected Conclusions (Weather Data)\n",
                "\n",
                "From exploring the weather dataset, you should observe:\n",
                "\n",
                "1. **January in NYC is cold** (highs 30-50Â°F, lows 15-35Â°F)\n",
                "2. **Mix of clear and precipitation days** typical for winter\n",
                "3. **Snow days** are distinct and could affect transportation\n",
                "4. **Date column** enables joining with taxi data on pickup_date\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ‹ï¸ Exercise 3: Count Snow Days\n",
                "\n",
                "**Task:** Count how many days had snowfall greater than 0.\n",
                "\n",
                "**Hint:** Filter where `snow > 0`, then count with `len()` or `.shape[0]`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Count days with snowfall > 0\n",
                "# Replace 'None' with your code\n",
                "\n",
                "snow_days = None  # YOUR CODE HERE\n",
                "\n",
                "# Uncomment to check your answer:\n",
                "# print(f\"Number of snow days: {snow_days}\")\n",
                "\n",
                "# Expected: Around 5-8 days in January"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# 5ï¸âƒ£ Air Quality Index Dataset\n",
                "\n",
                "## Background\n",
                "\n",
                "The EPA (Environmental Protection Agency) monitors air quality across US cities. The AQI (Air Quality Index) ranges from 0-500:\n",
                "\n",
                "| AQI Range | Category | Health Implications |\n",
                "|-----------|----------|---------------------|\n",
                "| 0-50 | Good | Air quality is satisfactory |\n",
                "| 51-100 | Moderate | Acceptable; risk for sensitive groups |\n",
                "| 101-150 | Unhealthy for Sensitive Groups | General public OK |\n",
                "| 151-200 | Unhealthy | Everyone may experience effects |\n",
                "| 201-300 | Very Unhealthy | Health alert |\n",
                "| 301-500 | Hazardous | Emergency conditions |\n",
                "\n",
                "**Smart City Use Cases:**\n",
                "- Issue health advisories\n",
                "- Identify pollution sources\n",
                "- Plan outdoor events based on air quality"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Air Quality dataset\n",
                "aqi_df = pd.read_csv(AQI_URL)\n",
                "\n",
                "print(f\"âœ… Loaded {len(aqi_df)} AQI records\")\n",
                "print(f\"ðŸ“ Cities: {aqi_df['city'].nunique()} ({', '.join(aqi_df['city'].unique())})\")\n",
                "aqi_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Average AQI by city\n",
                "print(\"ðŸ™ï¸ Average Air Quality by City (lower is better):\")\n",
                "print(\"=\" * 50)\n",
                "city_aqi = aqi_df.groupby('city')['aqi_value'].mean().sort_values()\n",
                "for city, aqi in city_aqi.items():\n",
                "    print(f\"   {city:15} AQI: {aqi:.1f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ“ Expected Conclusions (AQI Data)\n",
                "\n",
                "From exploring the air quality dataset, you should observe:\n",
                "\n",
                "1. **Los Angeles has highest AQI** (worst air) due to smog\n",
                "2. **New York has moderate AQI** (better than LA, worse than Phoenix)\n",
                "3. **Most days are Good or Moderate** category\n",
                "4. **PM2.5 and Ozone** are common main pollutants\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ‹ï¸ Exercise 4: Find City with Worst Air Quality\n",
                "\n",
                "**Task:** Find the city with the highest average AQI (worst air quality).\n",
                "\n",
                "**Hint:** Group by `city`, calculate `mean()` of `aqi_value`, find `idxmax()`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Find city with highest average AQI\n",
                "# Replace 'None' with your code\n",
                "\n",
                "worst_aqi_city = None  # YOUR CODE HERE\n",
                "\n",
                "# Uncomment to check your answer:\n",
                "# print(f\"City with worst air quality: {worst_aqi_city}\")\n",
                "\n",
                "# Expected: Los Angeles (typically has highest AQI due to smog)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# 6ï¸âƒ£ Data Joins Preview\n",
                "\n",
                "## The Power of Combining Datasets\n",
                "\n",
                "Real insights come from **joining multiple datasets**. For example:\n",
                "\n",
                "- *\"How does rain affect taxi tips?\"*\n",
                "- *\"Do crime rates increase on hot days?\"*\n",
                "- *\"Does poor air quality reduce taxi ridership?\"*\n",
                "\n",
                "Let's preview how to join taxi and weather data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: Extract date from taxi pickup datetime\n",
                "taxi_df['pickup_date'] = pd.to_datetime(taxi_df['tpep_pickup_datetime']).dt.date\n",
                "\n",
                "# Step 2: Create date column in weather for matching\n",
                "weather_df['date_only'] = weather_df['date'].dt.date\n",
                "\n",
                "# Step 3: Join taxi with weather on date\n",
                "joined_df = taxi_df.merge(\n",
                "    weather_df[['date_only', 'temp_avg', 'precipitation', 'weather_condition']],\n",
                "    left_on='pickup_date',\n",
                "    right_on='date_only',\n",
                "    how='left'  # Keep all taxi records, add weather where available\n",
                ")\n",
                "\n",
                "print(f\"âœ… Joined dataset: {len(joined_df)} records\")\n",
                "print(f\"ðŸ“Š Original taxi records: {len(taxi_df)}\")\n",
                "print(f\"ðŸŒ¤ï¸ Records with weather data: {joined_df['temp_avg'].notna().sum()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# View joined data\n",
                "joined_df[['tpep_pickup_datetime', 'fare_amount', 'tip_amount', 'temp_avg', 'weather_condition']].head(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze: Do tips vary by weather condition?\n",
                "# (Note: Sample size is small, so results may not be statistically significant)\n",
                "print(\"ðŸ’¡ Average Tip by Weather Condition:\")\n",
                "print(\"=\" * 40)\n",
                "tip_by_weather = joined_df.groupby('weather_condition')['tip_amount'].mean().sort_values(ascending=False)\n",
                "for condition, avg_tip in tip_by_weather.items():\n",
                "    print(f\"   {condition:15} ${avg_tip:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ“ Expected Conclusions (Joins)\n",
                "\n",
                "From the join preview:\n",
                "\n",
                "1. **Joining on date** is a common pattern for cross-dataset analysis\n",
                "2. **Weather affects tipping** (you might see higher tips on rainy/snowy days)\n",
                "3. **Sample size matters** - with 50 records, we can't draw strong conclusions\n",
                "4. **This is why we need Big Data!** - Millions of records reveal real patterns\n",
                "\n",
                "> **ðŸ“… Milestone 4 (Spark):** You'll perform this join on the **full datasets** with millions of records!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# 7ï¸âƒ£ Traditional Techniques: Why They Fail at Scale âš ï¸\n",
                "\n",
                "Now comes the most important part of this lab: understanding **why you need Big Data tools**.\n",
                "\n",
                "Let's demonstrate three fundamental problems with traditional single-machine approaches:\n",
                "\n",
                "1. **Algorithmic complexity** (O(nÂ²) becomes hours)\n",
                "2. **Memory limits** (can't fit data in RAM)\n",
                "3. **Single-threaded processing** (only uses 1 CPU core)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Problem 1: O(nÂ²) Nested Loops\n",
                "\n",
                "Many data operations require comparing every record to every other record (e.g., finding duplicates, computing similarity). With nested loops, the time grows **quadratically**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def naive_comparison(df):\n",
                "    \"\"\"O(nÂ²) algorithm - compare every record to every other.\"\"\"\n",
                "    count = 0\n",
                "    for i in range(len(df)):\n",
                "        for j in range(len(df)):\n",
                "            count += 1  # Simulate comparison operation\n",
                "    return count\n",
                "\n",
                "# Time with our 50 records\n",
                "start = time.time()\n",
                "comparisons = naive_comparison(taxi_df)\n",
                "time_50 = time.time() - start\n",
                "\n",
                "print(\"â±ï¸ O(nÂ²) Complexity Demonstration\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"With {len(taxi_df)} records:\")\n",
                "print(f\"   Comparisons: {comparisons:,}\")\n",
                "print(f\"   Time: {time_50:.4f} seconds\")\n",
                "\n",
                "# Extrapolate to real-world scale\n",
                "scale_factor = (1_000_000 / len(taxi_df)) ** 2\n",
                "estimated_time = time_50 * scale_factor\n",
                "\n",
                "print(f\"\\nâš ï¸ Extrapolated to 1 MILLION records:\")\n",
                "print(f\"   Comparisons: 1,000,000,000,000 (1 TRILLION!)\")\n",
                "print(f\"   Estimated time: {estimated_time / 3600:.1f} HOURS\")\n",
                "print(f\"\\nðŸ’¡ Solution: MapReduce distributes this across 100+ machines!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Problem 2: Memory Limits\n",
                "\n",
                "A typical laptop has 8-16 GB of RAM. What happens when your data is larger?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate memory usage\n",
                "sample_memory_kb = taxi_df.memory_usage(deep=True).sum() / 1024\n",
                "records_per_gb = (1024 * 1024) / sample_memory_kb * len(taxi_df)\n",
                "\n",
                "print(\"ðŸ’¾ Memory Analysis\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Sample ({len(taxi_df)} records): {sample_memory_kb:.2f} KB\")\n",
                "print(f\"Memory per record: {sample_memory_kb * 1024 / len(taxi_df):.0f} bytes\")\n",
                "print(f\"Records that fit in 1 GB RAM: ~{records_per_gb:,.0f}\")\n",
                "\n",
                "print(f\"\\nâš ï¸ Full NYC Taxi dataset (100+ million records):\")\n",
                "estimated_gb = 100_000_000 / records_per_gb\n",
                "print(f\"   Would require ~{estimated_gb:.0f} GB RAM!\")\n",
                "print(f\"   Your laptop probably has: 8-16 GB\")\n",
                "print(f\"\\nðŸ’¡ Solution: HDFS distributes data across many machines!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Problem 3: Single-Threaded Processing\n",
                "\n",
                "Pandas runs on a single CPU core. Modern servers have 64+ cores sitting idle!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a larger test dataset (replicate our sample)\n",
                "large_df = pd.concat([taxi_df] * 100, ignore_index=True)  # 5,000 records\n",
                "\n",
                "# Single-threaded aggregation\n",
                "start = time.time()\n",
                "result = large_df.groupby('PULocationID')['fare_amount'].agg(['sum', 'mean', 'count'])\n",
                "single_time = time.time() - start\n",
                "\n",
                "print(\"ðŸ–¥ï¸ Single-Threaded Processing\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Aggregation on {len(large_df):,} records: {single_time:.4f} seconds\")\n",
                "\n",
                "# Extrapolate\n",
                "estimated_100m = single_time * (100_000_000 / len(large_df))\n",
                "print(f\"\\nâš ï¸ Extrapolated to 100 MILLION records:\")\n",
                "print(f\"   Single core: ~{estimated_100m:.0f} seconds ({estimated_100m/60:.1f} minutes)\")\n",
                "print(f\"   100 Spark cores: ~{estimated_100m/100:.1f} seconds\")\n",
                "print(f\"\\nðŸ’¡ Solution: Spark parallelizes across all available cores!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“ Summary: Why Big Data Tools?\n",
                "\n",
                "| Problem | Traditional Approach | Big Data Solution |\n",
                "|---------|---------------------|-------------------|\n",
                "| **O(nÂ²) algorithms** | Takes hours/days | **MapReduce** parallelizes across nodes |\n",
                "| **Memory limits** | Can't load data | **HDFS** distributes storage across cluster |\n",
                "| **Single CPU core** | Wastes resources | **Spark** uses all cores on all machines |\n",
                "| **No fault tolerance** | Start over on crash | **Hadoop** automatically recovers from failures |\n",
                "\n",
                "> **ðŸ’¡ Key Insight:** The same code you wrote today will work on massive datasets when you run it on Spark/Databricks. The APIs are similarâ€”the infrastructure does the heavy lifting!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# 8ï¸âƒ£ Final Practice Exercises\n",
                "\n",
                "Complete these exercises to solidify your understanding. Each uses techniques from earlier sections."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ‹ï¸ Exercise 5: Filter Long Trips\n",
                "\n",
                "**Task:** Create a DataFrame containing only taxi trips longer than 5 miles."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Filter trips where trip_distance > 5\n",
                "# Replace 'None' with your code\n",
                "\n",
                "long_trips = None  # YOUR CODE HERE\n",
                "\n",
                "# Uncomment to check:\n",
                "# print(f\"Long trips (>5 miles): {len(long_trips)}\")\n",
                "# print(f\"Average fare for long trips: ${long_trips['fare_amount'].mean():.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ‹ï¸ Exercise 6: Crimes by District\n",
                "\n",
                "**Task:** Count the number of crimes in each police district."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Count crimes by District\n",
                "# Replace 'None' with your code\n",
                "\n",
                "crimes_by_district = None  # YOUR CODE HERE\n",
                "\n",
                "# Uncomment to check:\n",
                "# print(\"Crimes by District:\")\n",
                "# print(crimes_by_district)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ðŸ‹ï¸ Exercise 7: Rainy Day Temperature\n",
                "\n",
                "**Task:** Calculate the average temperature on days with precipitation > 0.1 inches."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Average temp_avg on days where precipitation > 0.1\n",
                "# Replace 'None' with your code\n",
                "\n",
                "rainy_avg_temp = None  # YOUR CODE HERE\n",
                "\n",
                "# Uncomment to check:\n",
                "# print(f\"Average temperature on rainy days: {rainy_avg_temp:.1f}Â°F\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# âœ… Lab Complete!\n",
                "\n",
                "## ðŸŽ“ What You Learned\n",
                "\n",
                "| Skill | Functions Used |\n",
                "|-------|----------------|\n",
                "| **Loading data** | `pd.read_csv()` |\n",
                "| **Exploring structure** | `info()`, `head()`, `describe()` |\n",
                "| **Handling missing data** | `isnull().sum()` |\n",
                "| **Filtering** | Boolean indexing: `df[df['col'] > value]` |\n",
                "| **Aggregating** | `groupby()`, `mean()`, `sum()`, `count()` |\n",
                "| **Joining datasets** | `merge()` |\n",
                "| **Understanding scale issues** | Time/memory extrapolation |\n",
                "\n",
                "## ðŸ“… What's Next?\n",
                "\n",
                "| Week | Topic | What You'll Learn |\n",
                "|------|-------|------------------|\n",
                "| 2 | Big Data & HDFS | The 5 V's, distributed storage |\n",
                "| 3-4 | M1: Data Loading | Load datasets into HDFS/Databricks |\n",
                "| 5-6 | M2: MapReduce | Parallel batch processing |\n",
                "| 7-8 | M3: Hive | SQL on Big Data |\n",
                "| 9-10 | M4: Spark | Fast joins across datasets |\n",
                "| 11-12 | M5: Streaming | Real-time data pipelines |\n",
                "\n",
                "## ðŸ”— Resources\n",
                "\n",
                "**Full Datasets (Kaggle):**\n",
                "- [NYC Yellow Taxi](https://www.kaggle.com/datasets/elemento/nyc-yellow-taxi-trip-data)\n",
                "- [Chicago Crimes](https://www.kaggle.com/datasets/chicago/chicago-crime)\n",
                "- [NYC Weather](https://www.kaggle.com/datasets/danbraswell/new-york-city-weather-data-2019)\n",
                "- [Air Quality](https://www.kaggle.com/datasets/programmerrdai/air-quality-data-2012-to-2024)\n",
                "\n",
                "**Course Repository:**\n",
                "- [github.com/aniskoubaa/big_data_course](https://github.com/aniskoubaa/big_data_course)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# ðŸ“– Pandas Cheatsheet\n",
                "\n",
                "A quick reference for students who are new to Pandas or need a refresher. Each section includes a brief explanation and example code."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Loading Data\n",
                "\n",
                "**What it does:** Reads data from a file (CSV, Excel, etc.) into a Pandas DataFrameâ€”a table-like structure with rows and columns.\n",
                "\n",
                "**When to use:** At the start of any data analysis to import your data.\n",
                "\n",
                "```python\n",
                "import pandas as pd\n",
                "\n",
                "# From local file\n",
                "df = pd.read_csv('data.csv')\n",
                "\n",
                "# From URL (works in Colab!)\n",
                "df = pd.read_csv('https://example.com/data.csv')\n",
                "```\n",
                "\n",
                "> ðŸ’¡ **Tip:** The variable `df` is a common convention for \"DataFrame\"."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Exploring Data\n",
                "\n",
                "**What it does:** Gives you a quick overview of your datasetâ€”how big it is, what columns exist, and basic statistics.\n",
                "\n",
                "**When to use:** Right after loading data to understand its structure.\n",
                "\n",
                "```python\n",
                "df.head()        # View first 5 rows (quick peek)\n",
                "df.head(10)      # View first 10 rows\n",
                "df.shape         # Returns (rows, columns) tuple\n",
                "df.columns       # List all column names\n",
                "df.info()        # Data types + non-null counts\n",
                "df.describe()    # Statistics: mean, std, min, max, quartiles\n",
                "```\n",
                "\n",
                "> ðŸ’¡ **Tip:** `info()` shows memory usage and helps identify missing values."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Selecting Columns\n",
                "\n",
                "**What it does:** Extracts one or more columns from your DataFrame.\n",
                "\n",
                "**When to use:** When you want to focus on specific variables.\n",
                "\n",
                "```python\n",
                "# One column â†’ returns a Series (1D)\n",
                "df['fare_amount']\n",
                "\n",
                "# Multiple columns â†’ returns a DataFrame (2D)\n",
                "df[['fare_amount', 'tip_amount']]\n",
                "```\n",
                "\n",
                "> ðŸ’¡ **Tip:** Use double brackets `[[ ]]` for multiple columns."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Filtering Rows\n",
                "\n",
                "**What it does:** Selects only rows that meet a condition (like SQL's `WHERE`).\n",
                "\n",
                "**When to use:** To analyze a subset of your data.\n",
                "\n",
                "```python\n",
                "# Basic filter: trips longer than 5 miles\n",
                "df[df['trip_distance'] > 5]\n",
                "\n",
                "# Multiple conditions: use & (AND) or | (OR)\n",
                "df[(df['fare'] > 10) & (df['tip'] > 2)]\n",
                "\n",
                "# Check if value is in a list\n",
                "df[df['city'].isin(['NYC', 'LA', 'Chicago'])]\n",
                "```\n",
                "\n",
                "> âš ï¸ **Common mistake:** Forgetting parentheses around each condition when using `&` or `|`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Aggregations\n",
                "\n",
                "**What it does:** Calculates summary statistics like averages, totals, or counts.\n",
                "\n",
                "**When to use:** To answer questions like \"What's the average fare?\" or \"Total tips by city?\"\n",
                "\n",
                "```python\n",
                "# Single column aggregations\n",
                "df['fare'].mean()     # Average\n",
                "df['fare'].sum()      # Total\n",
                "df['fare'].max()      # Maximum value\n",
                "df['fare'].min()      # Minimum value\n",
                "\n",
                "# Group by category, then aggregate\n",
                "df.groupby('city')['fare'].mean()    # Avg fare per city\n",
                "df.groupby('city')['fare'].sum()     # Total fare per city\n",
                "```\n",
                "\n",
                "> ðŸ’¡ **Tip:** `groupby()` is like SQL's `GROUP BY`â€”splits data, applies function, combines results."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Counting & Finding Most Common\n",
                "\n",
                "**What it does:** Counts how often each unique value appears in a column.\n",
                "\n",
                "**When to use:** To find the most common crime type, busiest location, etc.\n",
                "\n",
                "```python\n",
                "# Count each unique value (sorted by frequency)\n",
                "df['crime_type'].value_counts()\n",
                "\n",
                "# Get the MOST common value (the name/index)\n",
                "df['crime_type'].value_counts().idxmax()\n",
                "\n",
                "# Count unique values\n",
                "df['city'].nunique()  # How many different cities?\n",
                "```\n",
                "\n",
                "> ðŸ’¡ **Tip:** `idxmax()` returns the *label* of the max value, not the max itself."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Handling Missing Values\n",
                "\n",
                "**What it does:** Detects null/missing values (NaN) in your data.\n",
                "\n",
                "**When to use:** Before analysis to check data quality.\n",
                "\n",
                "```python\n",
                "# Count nulls per column\n",
                "df.isnull().sum()\n",
                "\n",
                "# Check if a specific column has nulls\n",
                "df['tip'].isna().sum()\n",
                "\n",
                "# Filter to rows WITHOUT nulls\n",
                "df[df['tip'].notna()]\n",
                "```\n",
                "\n",
                "> ðŸ’¡ **Tip:** `isnull()` and `isna()` are interchangeable."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Working with Dates\n",
                "\n",
                "**What it does:** Converts strings to proper datetime objects and extracts parts (year, month, day).\n",
                "\n",
                "**When to use:** When analyzing time-series data or joining on dates.\n",
                "\n",
                "```python\n",
                "# Convert string to datetime\n",
                "df['date'] = pd.to_datetime(df['date'])\n",
                "\n",
                "# Extract components\n",
                "df['date'].dt.date       # Just the date (no time)\n",
                "df['date'].dt.year       # Year (2024)\n",
                "df['date'].dt.month      # Month (1-12)\n",
                "df['date'].dt.day        # Day (1-31)\n",
                "df['date'].dt.hour       # Hour (0-23)\n",
                "```\n",
                "\n",
                "> ðŸ’¡ **Tip:** Use `.dt.date` when joining taxi data with weather data on date."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Joining DataFrames\n",
                "\n",
                "**What it does:** Combines two DataFrames based on a common column (like SQL JOIN).\n",
                "\n",
                "**When to use:** To combine taxi trips with weather data, or crimes with location info.\n",
                "\n",
                "```python\n",
                "# Basic merge on shared column\n",
                "merged = taxi_df.merge(weather_df, on='date')\n",
                "\n",
                "# Different column names in each DataFrame\n",
                "merged = df1.merge(df2, left_on='pickup_date', right_on='date')\n",
                "\n",
                "# Keep all rows from left, add matching from right\n",
                "merged = df1.merge(df2, on='key', how='left')\n",
                "```\n",
                "\n",
                "> ðŸ’¡ **Tip:** `how='left'` keeps all left rows even if no match exists (fills with NaN)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Exercise Solution Patterns\n",
                "\n",
                "**Common patterns you'll need for the exercises:**\n",
                "\n",
                "```python\n",
                "# Calculate percentage\n",
                "avg_tip_pct = (df['tip'] / df['fare'] * 100).mean()\n",
                "\n",
                "# Find most common category\n",
                "most_common = df['category'].value_counts().idxmax()\n",
                "\n",
                "# Count rows matching a condition\n",
                "snow_days = len(df[df['snow'] > 0])\n",
                "# Alternative:\n",
                "snow_days = (df['snow'] > 0).sum()\n",
                "\n",
                "# Average of a filtered subset\n",
                "rainy_temp = df[df['precipitation'] > 0.1]['temp_avg'].mean()\n",
                "\n",
                "# Find group with highest average\n",
                "worst_city = df.groupby('city')['aqi'].mean().idxmax()\n",
                "```\n",
                "\n",
                "> ðŸŽ¯ **Use these patterns to solve the exercises above!**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
