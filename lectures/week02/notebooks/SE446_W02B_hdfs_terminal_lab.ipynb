{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41785b56",
   "metadata": {},
   "source": [
    "# SE446 â€“ Week 2B: HDFS Command Line Lab\n",
    "\n",
    "## ğŸ–¥ï¸ Hands-On Terminal Practice with HDFS\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Connect to a remote Hadoop cluster** via SSH\n",
    "2. **Navigate HDFS** using command-line tools\n",
    "3. **Create directories and upload files** to HDFS\n",
    "4. **View and manage files** in the distributed file system\n",
    "5. **Check cluster status** using admin commands\n",
    "\n",
    "---\n",
    "\n",
    "### â±ï¸ Estimated Time: 30-45 minutes\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“‹ Prerequisites\n",
    "\n",
    "- Basic knowledge of Linux terminal commands\n",
    "- SSH credentials (provided by instructor)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒ Our HDFS Cluster\n",
    "\n",
    "| Component | Value |\n",
    "|-----------|-------|\n",
    "| **Master Node** | 134.209.172.50 |\n",
    "| **Hadoop Version** | 3.4.1 |\n",
    "| **Total DataNodes** | 2 |\n",
    "| **Replication Factor** | 2 |\n",
    "| **Web UI** | https://hdfs.aniskoubaa.org |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc2fd1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ Part 1: Environment Setup\n",
    "\n",
    "### 1.1 Install SSH Client (Colab Only)\n",
    "\n",
    "Google Colab doesn't have `sshpass` installed by default. Let's install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sshpass for password-based SSH (Colab)\n",
    "!apt-get update -qq && apt-get install -qq sshpass > /dev/null 2>&1\n",
    "print(\"âœ… SSH tools installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ee85af",
   "metadata": {},
   "source": [
    "### 1.2 Configure Your Credentials\n",
    "\n",
    "**âš ï¸ Important**: Ask your instructor for the SSH password.\n",
    "\n",
    "#### For Google Colab:\n",
    "1. Click the ğŸ”‘ **Secrets** icon in the left sidebar\n",
    "2. Add a secret named `HDFS_PASSWORD` with the password\n",
    "3. Toggle \"Notebook access\" ON\n",
    "\n",
    "#### For Local Jupyter:\n",
    "Create a `.env` file or enter the password when prompted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc019067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Cluster connection details\n",
    "MASTER_HOST = \"134.209.172.50\"\n",
    "SSH_USER = \"root\"\n",
    "HADOOP_USER = \"hadoop\"\n",
    "\n",
    "# Try to load password from Colab secrets or environment\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    SSH_PASSWORD = userdata.get('HDFS_PASSWORD')\n",
    "    print(\"ğŸŒ Running in Google Colab\")\n",
    "    print(\"âœ… Password loaded from Colab Secrets\")\n",
    "except:\n",
    "    # Try environment variable\n",
    "    SSH_PASSWORD = os.getenv('HDFS_SSH_PASSWORD')\n",
    "    if SSH_PASSWORD:\n",
    "        print(\"ğŸ’» Running locally\")\n",
    "        print(\"âœ… Password loaded from environment\")\n",
    "    else:\n",
    "        # Manual input (not recommended for shared notebooks)\n",
    "        from getpass import getpass\n",
    "        SSH_PASSWORD = getpass(\"Enter SSH password: \")\n",
    "        print(\"âœ… Password entered manually\")\n",
    "\n",
    "# Store in environment for shell commands\n",
    "os.environ['SSHPASS'] = SSH_PASSWORD\n",
    "os.environ['HDFS_HOST'] = MASTER_HOST\n",
    "\n",
    "print(f\"\\nğŸ“ Target: {SSH_USER}@{MASTER_HOST}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e2a69",
   "metadata": {},
   "source": [
    "### 1.3 Create SSH Helper Function\n",
    "\n",
    "This helper makes it easy to run commands on the remote cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafcd5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssh_run(command, as_hadoop=True):\n",
    "    \"\"\"\n",
    "    Run a command on the remote HDFS cluster via SSH.\n",
    "    \n",
    "    Args:\n",
    "        command: The command to execute\n",
    "        as_hadoop: If True, run as hadoop user (for HDFS commands)\n",
    "    \"\"\"\n",
    "    if as_hadoop:\n",
    "        remote_cmd = f\"sudo -u hadoop {command}\"\n",
    "    else:\n",
    "        remote_cmd = command\n",
    "    \n",
    "    full_cmd = f'sshpass -e ssh -o StrictHostKeyChecking=no {SSH_USER}@{MASTER_HOST} \"{remote_cmd}\"'\n",
    "    return full_cmd\n",
    "\n",
    "print(\"âœ… SSH helper function ready!\")\n",
    "print(\"\\nğŸ“ Usage: !{ssh_run('hdfs dfs -ls /')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710337e6",
   "metadata": {},
   "source": [
    "### 1.4 Test Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f84151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SSH connection\n",
    "print(\"ğŸ”Œ Testing connection to HDFS cluster...\")\n",
    "print(\"=\" * 50)\n",
    "!{ssh_run('echo \"Connection successful! Hostname: $(hostname)\"', as_hadoop=False)}\n",
    "print(\"\\nâœ… You are connected to the cluster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b120e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“– Part 2: HDFS Command Basics\n",
    "\n",
    "### 2.1 Understanding HDFS Commands\n",
    "\n",
    "All HDFS file operations use the pattern:\n",
    "\n",
    "```bash\n",
    "hdfs dfs -<command> [arguments]\n",
    "```\n",
    "\n",
    "**Common Commands:**\n",
    "\n",
    "| Command | Description | Example |\n",
    "|---------|-------------|----------|\n",
    "| `-ls` | List files | `hdfs dfs -ls /` |\n",
    "| `-mkdir` | Create directory | `hdfs dfs -mkdir /mydir` |\n",
    "| `-put` | Upload file | `hdfs dfs -put file.txt /mydir/` |\n",
    "| `-get` | Download file | `hdfs dfs -get /mydir/file.txt .` |\n",
    "| `-cat` | View file content | `hdfs dfs -cat /mydir/file.txt` |\n",
    "| `-rm` | Delete file | `hdfs dfs -rm /mydir/file.txt` |\n",
    "| `-du` | Check disk usage | `hdfs dfs -du -h /` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3a8a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§ª Part 3: Hands-On Exercises\n",
    "\n",
    "### Exercise 1: Check Hadoop Version\n",
    "\n",
    "**Task**: Verify the Hadoop installation by checking its version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Hadoop version\n",
    "print(\"ğŸ“‹ HADOOP VERSION\")\n",
    "print(\"=\" * 40)\n",
    "!{ssh_run('hadoop version', as_hadoop=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be89c920",
   "metadata": {},
   "source": [
    "**ğŸ“ Question 1**: What version of Hadoop is installed on the cluster?\n",
    "\n",
    "> Your answer: _________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e768bf12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 2: List Root Directory\n",
    "\n",
    "**Task**: View the contents of the HDFS root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20895fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the HDFS root directory\n",
    "print(\"ğŸ“ HDFS ROOT DIRECTORY (/)\")\n",
    "print(\"=\" * 50)\n",
    "!{ssh_run('hdfs dfs -ls /')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e03744e",
   "metadata": {},
   "source": [
    "**Understanding the Output:**\n",
    "\n",
    "```\n",
    "drwxr-xr-x   - hadoop supergroup    0 2024-01-15 10:30 /user\n",
    "â”‚â””â”¬â”˜â””â”¬â”˜â””â”¬â”˜    â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”¬â”€â”˜\n",
    "â”‚ â”‚  â”‚  â”‚        â”‚        â”‚       â”‚         â”‚            â”‚\n",
    "â”‚ â”‚  â”‚  â”‚        â”‚        â”‚       â”‚         â”‚            â””â”€ Path\n",
    "â”‚ â”‚  â”‚  â”‚        â”‚        â”‚       â”‚         â””â”€ Modification time\n",
    "â”‚ â”‚  â”‚  â”‚        â”‚        â”‚       â””â”€ Size (0 for directories)\n",
    "â”‚ â”‚  â”‚  â”‚        â”‚        â””â”€ Group\n",
    "â”‚ â”‚  â”‚  â”‚        â””â”€ Owner\n",
    "â”‚ â”‚  â”‚  â””â”€ Execute permission\n",
    "â”‚ â”‚  â””â”€ Write permission\n",
    "â”‚ â””â”€ Read permission\n",
    "â””â”€ d = directory, - = file\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fbc44d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 3: Check Cluster Status\n",
    "\n",
    "**Task**: Get a report of the HDFS cluster health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8051a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HDFS cluster report\n",
    "print(\"ğŸ“Š HDFS CLUSTER REPORT\")\n",
    "print(\"=\" * 60)\n",
    "!{ssh_run('hdfs dfsadmin -report')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc9303c",
   "metadata": {},
   "source": [
    "**ğŸ“ Question 2**: Fill in the table based on the cluster report:\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Live DataNodes | _____ |\n",
    "| Total Capacity | _____ GB |\n",
    "| DFS Used | _____ % |\n",
    "| DFS Remaining | _____ GB |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c4c7b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 4: Create Your Directory\n",
    "\n",
    "**Task**: Create a personal directory in HDFS.\n",
    "\n",
    "**âš ï¸ Replace `YOUR_NAME` with your actual name (no spaces)!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe45b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ğŸ‘‡ CHANGE THIS TO YOUR NAME (no spaces!) ğŸ‘‡\n",
    "# ============================================\n",
    "STUDENT_NAME = \"student_demo\"\n",
    "# ============================================\n",
    "\n",
    "print(f\"ğŸ“ Creating directory for: {STUDENT_NAME}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create the directory\n",
    "!{ssh_run(f'hdfs dfs -mkdir -p /students/{STUDENT_NAME}')}\n",
    "print(f\"âœ… Created: /students/{STUDENT_NAME}\")\n",
    "\n",
    "# Verify\n",
    "print(\"\\nğŸ“‹ Verifying directory exists:\")\n",
    "!{ssh_run('hdfs dfs -ls /students')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9ce36",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 5: Create and Upload a File\n",
    "\n",
    "**Task**: Create a text file and upload it to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a sample file on the server\n",
    "print(\"ğŸ“ STEP 1: Creating a sample file\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sample_content = f\"Hello from {STUDENT_NAME}!\\\\nThis is my first HDFS file.\\\\nCreated during SE446 Lab.\"\n",
    "!{ssh_run(f'echo -e \"{sample_content}\" > /tmp/hello_{STUDENT_NAME}.txt', as_hadoop=False)}\n",
    "\n",
    "# Verify local file\n",
    "print(\"Local file content:\")\n",
    "!{ssh_run(f'cat /tmp/hello_{STUDENT_NAME}.txt', as_hadoop=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d7aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Upload to HDFS\n",
    "print(\"\\nğŸ“¤ STEP 2: Uploading to HDFS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "!{ssh_run(f'hdfs dfs -put /tmp/hello_{STUDENT_NAME}.txt /students/{STUDENT_NAME}/')}\n",
    "print(\"âœ… File uploaded!\")\n",
    "\n",
    "# Verify upload\n",
    "print(f\"\\nğŸ“‹ Files in /students/{STUDENT_NAME}/:\")\n",
    "!{ssh_run(f'hdfs dfs -ls /students/{STUDENT_NAME}/')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ed39b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 6: View File Content\n",
    "\n",
    "**Task**: Read your file directly from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aa6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View file content using -cat\n",
    "print(\"ğŸ“„ FILE CONTENT FROM HDFS\")\n",
    "print(\"=\" * 50)\n",
    "!{ssh_run(f'hdfs dfs -cat /students/{STUDENT_NAME}/hello_{STUDENT_NAME}.txt')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd21a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 7: Check Disk Usage\n",
    "\n",
    "**Task**: Check how much space your files are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eedc7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check disk usage\n",
    "print(\"ğŸ“Š DISK USAGE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Your directory\n",
    "print(f\"\\nğŸ“ Your directory (/students/{STUDENT_NAME}):\")\n",
    "!{ssh_run(f'hdfs dfs -du -h /students/{STUDENT_NAME}')}\n",
    "\n",
    "# Overall HDFS usage\n",
    "print(\"\\nğŸ“ Overall HDFS usage:\")\n",
    "!{ssh_run('hdfs dfs -df -h')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a391a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 8: File Replication Info\n",
    "\n",
    "**Task**: Check the replication factor and block locations of your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06239c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check file statistics\n",
    "print(\"ğŸ“Š FILE STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get replication factor\n",
    "print(\"\\nğŸ”¢ Replication factor:\")\n",
    "!{ssh_run(f\"hdfs dfs -stat '%r' /students/{STUDENT_NAME}/hello_{STUDENT_NAME}.txt\")}\n",
    "\n",
    "# Get block size\n",
    "print(\"\\nğŸ“¦ Block size:\")\n",
    "!{ssh_run(f\"hdfs dfs -stat '%o' /students/{STUDENT_NAME}/hello_{STUDENT_NAME}.txt\")}\n",
    "\n",
    "# Get file size\n",
    "print(\"\\nğŸ“ File size (bytes):\")\n",
    "!{ssh_run(f\"hdfs dfs -stat '%b' /students/{STUDENT_NAME}/hello_{STUDENT_NAME}.txt\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e9cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which DataNodes store your file\n",
    "print(\"ğŸ“ BLOCK LOCATIONS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nWhich DataNodes have copies of your file?\")\n",
    "!{ssh_run(f'hdfs fsck /students/{STUDENT_NAME}/hello_{STUDENT_NAME}.txt -files -blocks -locations')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b535238",
   "metadata": {},
   "source": [
    "**ğŸ“ Question 3**: On how many DataNodes is your file stored?\n",
    "\n",
    "> Your answer: _________________\n",
    "\n",
    "**ğŸ“ Question 4**: Why is the file replicated to multiple nodes?\n",
    "\n",
    "> Your answer: _________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5d00a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 9: Copy Files Within HDFS\n",
    "\n",
    "**Task**: Create a backup copy of your file within HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21566808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy file within HDFS\n",
    "print(\"ğŸ“‹ COPYING FILE WITHIN HDFS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create backup directory\n",
    "!{ssh_run(f'hdfs dfs -mkdir -p /students/{STUDENT_NAME}/backup')}\n",
    "\n",
    "# Copy file\n",
    "!{ssh_run(f'hdfs dfs -cp /students/{STUDENT_NAME}/hello_{STUDENT_NAME}.txt /students/{STUDENT_NAME}/backup/')}\n",
    "print(\"âœ… File copied!\")\n",
    "\n",
    "# Verify\n",
    "print(f\"\\nğŸ“‹ All files in /students/{STUDENT_NAME}/:\")\n",
    "!{ssh_run(f'hdfs dfs -ls -R /students/{STUDENT_NAME}/')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7617b4f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 10: Download File from HDFS\n",
    "\n",
    "**Task**: Download your file from HDFS to the local server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download file from HDFS\n",
    "print(\"ğŸ“¥ DOWNLOADING FROM HDFS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Download using -get\n",
    "!{ssh_run(f'hdfs dfs -get /students/{STUDENT_NAME}/hello_{STUDENT_NAME}.txt /tmp/downloaded_{STUDENT_NAME}.txt')}\n",
    "print(\"âœ… File downloaded!\")\n",
    "\n",
    "# Verify download\n",
    "print(\"\\nğŸ“„ Downloaded file content:\")\n",
    "!{ssh_run(f'cat /tmp/downloaded_{STUDENT_NAME}.txt', as_hadoop=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb4b0ce",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 11: Safe Mode Check\n",
    "\n",
    "**Task**: Check if the NameNode is in safe mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e393afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check safe mode status\n",
    "print(\"ğŸ”’ SAFE MODE STATUS\")\n",
    "print(\"=\" * 50)\n",
    "!{ssh_run('hdfs dfsadmin -safemode get')}\n",
    "\n",
    "print(\"\\nğŸ’¡ What is Safe Mode?\")\n",
    "print(\"   â””â”€ During safe mode, HDFS is read-only\")\n",
    "print(\"   â””â”€ This happens during startup or maintenance\")\n",
    "print(\"   â””â”€ Normal status: 'Safe mode is OFF'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5262ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise 12: Create Sample Data for MapReduce\n",
    "\n",
    "**Task**: Prepare data that we'll use in the next lecture on MapReduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8fe41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample word count data\n",
    "print(\"ğŸ“ CREATING SAMPLE DATA FOR MAPREDUCE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create sample text file\n",
    "wordcount_data = \"\"\"Hello World\n",
    "Hello Hadoop\n",
    "Hello HDFS\n",
    "Hadoop is great\n",
    "HDFS stores data\n",
    "MapReduce processes data\n",
    "Hello Big Data\n",
    "Big Data is the future\"\"\"\n",
    "\n",
    "# Write to remote server\n",
    "!{ssh_run(f'echo \"{wordcount_data}\" > /tmp/wordcount_input.txt', as_hadoop=False)}\n",
    "\n",
    "# Upload to HDFS\n",
    "!{ssh_run(f'hdfs dfs -mkdir -p /students/{STUDENT_NAME}/mapreduce')}\n",
    "!{ssh_run(f'hdfs dfs -put /tmp/wordcount_input.txt /students/{STUDENT_NAME}/mapreduce/')}\n",
    "\n",
    "print(\"âœ… Sample data created!\")\n",
    "print(f\"\\nğŸ“‹ File: /students/{STUDENT_NAME}/mapreduce/wordcount_input.txt\")\n",
    "print(\"\\nğŸ“„ Content:\")\n",
    "!{ssh_run(f'hdfs dfs -cat /students/{STUDENT_NAME}/mapreduce/wordcount_input.txt')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4f66f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Part 4: Command Reference\n",
    "\n",
    "### Quick Reference Card\n",
    "\n",
    "```bash\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# HDFS COMMAND QUICK REFERENCE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# --- File Navigation ---\n",
    "hdfs dfs -ls /path              # List directory\n",
    "hdfs dfs -ls -R /path           # List recursively\n",
    "hdfs dfs -ls -h /path           # Human-readable sizes\n",
    "\n",
    "# --- Directory Operations ---\n",
    "hdfs dfs -mkdir /path           # Create directory\n",
    "hdfs dfs -mkdir -p /path/sub    # Create with parents\n",
    "\n",
    "# --- File Operations ---\n",
    "hdfs dfs -put local remote      # Upload file\n",
    "hdfs dfs -get remote local      # Download file\n",
    "hdfs dfs -cat /path/file        # View content\n",
    "hdfs dfs -head /path/file       # First 1KB\n",
    "hdfs dfs -tail /path/file       # Last 1KB\n",
    "hdfs dfs -cp src dst            # Copy within HDFS\n",
    "hdfs dfs -mv src dst            # Move/rename\n",
    "\n",
    "# --- Delete ---\n",
    "hdfs dfs -rm /path/file         # Delete file\n",
    "hdfs dfs -rm -r /path/dir       # Delete directory\n",
    "hdfs dfs -rm -skipTrash /file   # Delete permanently\n",
    "\n",
    "# --- Information ---\n",
    "hdfs dfs -du -h /path           # Disk usage\n",
    "hdfs dfs -df -h                 # Free space\n",
    "hdfs dfs -stat '%r %o %b' /f    # File stats\n",
    "hdfs dfs -count /path           # Count files/dirs\n",
    "\n",
    "# --- Admin Commands ---\n",
    "hdfs dfsadmin -report           # Cluster report\n",
    "hdfs dfsadmin -safemode get     # Safe mode status\n",
    "hdfs fsck /path                 # File system check\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f62f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§¹ Part 5: Cleanup (Optional)\n",
    "\n",
    "Optionally clean up your test files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ac4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all your files before cleanup\n",
    "print(\"ğŸ“‹ YOUR FILES IN HDFS\")\n",
    "print(\"=\" * 50)\n",
    "!{ssh_run(f'hdfs dfs -ls -R /students/{STUDENT_NAME}/')}\n",
    "\n",
    "# Uncomment to delete (be careful!)\n",
    "# !{ssh_run(f'hdfs dfs -rm -r /students/{STUDENT_NAME}')}\n",
    "# print(\"\\nğŸ—‘ï¸ Directory deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837ee244",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Lab Checklist\n",
    "\n",
    "Before submitting, make sure you completed:\n",
    "\n",
    "- [ ] Connected to the HDFS cluster via SSH\n",
    "- [ ] Checked Hadoop version\n",
    "- [ ] Listed root directory contents\n",
    "- [ ] Viewed cluster status report\n",
    "- [ ] Created your personal directory\n",
    "- [ ] Uploaded a file to HDFS\n",
    "- [ ] Viewed file content with `-cat`\n",
    "- [ ] Checked file replication and block locations\n",
    "- [ ] Copied files within HDFS\n",
    "- [ ] Downloaded a file from HDFS\n",
    "- [ ] Created sample data for MapReduce lab\n",
    "- [ ] Answered all questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59503b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Lab Submission\n",
    "\n",
    "### Your Answers\n",
    "\n",
    "Copy and fill in your answers below:\n",
    "\n",
    "```\n",
    "Student Name: ________________________\n",
    "Student ID:   ________________________\n",
    "\n",
    "Question 1 (Hadoop Version): ________________________\n",
    "\n",
    "Question 2 (Cluster Status):\n",
    "  - Live DataNodes: ________________________\n",
    "  - Total Capacity: ________________________\n",
    "  - DFS Used %:     ________________________\n",
    "  - DFS Remaining:  ________________________\n",
    "\n",
    "Question 3 (Replication): ________________________\n",
    "\n",
    "Question 4 (Why replication?): \n",
    "________________________\n",
    "________________________\n",
    "________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776a9bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "| Skill | Commands Used |\n",
    "|-------|---------------|\n",
    "| **Connect** to cluster | `ssh user@host` |\n",
    "| **List** files | `hdfs dfs -ls` |\n",
    "| **Create** directories | `hdfs dfs -mkdir` |\n",
    "| **Upload** files | `hdfs dfs -put` |\n",
    "| **Download** files | `hdfs dfs -get` |\n",
    "| **View** content | `hdfs dfs -cat` |\n",
    "| **Check** status | `hdfs dfsadmin -report` |\n",
    "| **Analyze** blocks | `hdfs fsck -files -blocks` |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Next Steps\n",
    "\n",
    "1. **Practice**: Try more commands from the reference card\n",
    "2. **Explore**: Visit the Web UI at https://hdfs.aniskoubaa.org\n",
    "3. **Prepare**: Next lecture covers MapReduce!\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ”— Resources:**\n",
    "- [HDFS Commands Guide](https://hadoop.apache.org/docs/r3.4.1/hadoop-project-dist/hadoop-common/FileSystemShell.html)\n",
    "- [HDFS Admin Commands](https://hadoop.apache.org/docs/r3.4.1/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html)\n",
    "- Cluster Web UI: https://hdfs.aniskoubaa.org"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
