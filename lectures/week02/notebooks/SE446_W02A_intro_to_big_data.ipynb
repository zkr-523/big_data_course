{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SE446 ‚Äì Week 2A Hands-On: Introduction to Big Data with Python\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "In this notebook, we will **simulate key Big Data concepts** using a smaller scale environment (your laptop/Colab). Even though we aren't using a massive cluster yet, the *principles* remain the same.\n",
        "\n",
        "**Learning Objectives:**\n",
        "1. **Volume**: Observe how increasing data size affects memory and processing time.\n",
        "2. **Variety**: Work with different data formats (CSV, Complex JSON, Unstructured Text).\n",
        "3. **Veracity**: Identify dirty data and implement a basic cleaning pipeline.\n",
        "4. **ETL vs ELT**: Understand the difference between transforming *before* loading vs *after* loading.\n",
        "5. **File Formats**: Compare the storage and performance differences between **Row-based (CSV)** and **Columnar (Parquet)** formats.\n",
        "6. **Pandas vs Spark**: Understand the difference between In-Memory and Distributed computing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /opt/anaconda3/envs/training/lib/python3.12/site-packages (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/envs/training/lib/python3.12/site-packages (from pandas) (2.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/training/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/training/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyarrow in /opt/anaconda3/envs/training/lib/python3.12/site-packages (23.0.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install pandas\n",
        "# !pip install pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pandas version: 3.0.0\n"
          ]
        }
      ],
      "source": [
        "# Setup - Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(f\"Pandas version: {pd.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Volume: The Challenge of Scale\n",
        "\n",
        "### Objective\n",
        "Big Data is often defined by **Volume**‚Äîdatasets so large they cannot fit into the memory of a single machine. Here, we will simulate this by generating a \"large\" local dataset (1 million rows).\n",
        "\n",
        "### What to do\n",
        "1. Run the code to generate a synthetic dataset representing user activity.\n",
        "2. Observe the **memory usage** reported by Pandas.\n",
        "3. Observe the **time taken** to perform a simple aggregation query.\n",
        "\n",
        "Try to imagine: *What would happen if this dataset were 1,000x larger?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 1,000,000 rows of data...\n",
            "Generation complete.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>event_type</th>\n",
              "      <th>amount</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15796</td>\n",
              "      <td>view</td>\n",
              "      <td>44.07</td>\n",
              "      <td>2025-01-01 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>861</td>\n",
              "      <td>click</td>\n",
              "      <td>11.96</td>\n",
              "      <td>2025-01-01 00:00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>76821</td>\n",
              "      <td>click</td>\n",
              "      <td>0.87</td>\n",
              "      <td>2025-01-01 00:00:02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54887</td>\n",
              "      <td>click</td>\n",
              "      <td>13.41</td>\n",
              "      <td>2025-01-01 00:00:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6266</td>\n",
              "      <td>click</td>\n",
              "      <td>40.25</td>\n",
              "      <td>2025-01-01 00:00:04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id event_type  amount           timestamp\n",
              "0    15796       view   44.07 2025-01-01 00:00:00\n",
              "1      861      click   11.96 2025-01-01 00:00:01\n",
              "2    76821      click    0.87 2025-01-01 00:00:02\n",
              "3    54887      click   13.41 2025-01-01 00:00:03\n",
              "4     6266      click   40.25 2025-01-01 00:00:04"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 2.1 Generate Synthetic Dataset (1 Million Rows)\n",
        "# ===============================================================\n",
        "\n",
        "# N represents the number of simulated user interaction events\n",
        "N = 1_000_000  # 1 million interactions\n",
        "\n",
        "print(f\"Generating {N:,} rows of data...\")\n",
        "np.random.seed(42)  # Ensures reproducible random results\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Create a DataFrame simulating user activity events\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# Create a DataFrame simulating user purchase events\n",
        "df = pd.DataFrame({\n",
        "    \"user_id\": np.random.randint(1, 100_000, size=N),\n",
        "    \"event_type\": np.random.choice([\"click\", \"view\", \"purchase\"], size=N, p=[0.6, 0.35, 0.05]),\n",
        "    \"amount\": np.round(np.random.exponential(scale=50, size=N), 2),\n",
        "    \"timestamp\": pd.date_range(\"2025-01-01\", periods=N, freq=\"s\")\n",
        "})\n",
        "\n",
        "print(\"Generation complete.\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Memory Usage ---\n",
            "<class 'pandas.DataFrame'>\n",
            "RangeIndex: 1000000 entries, 0 to 999999\n",
            "Data columns (total 4 columns):\n",
            " #   Column      Non-Null Count    Dtype         \n",
            "---  ------      --------------    -----         \n",
            " 0   user_id     1000000 non-null  int64         \n",
            " 1   event_type  1000000 non-null  str           \n",
            " 2   amount      1000000 non-null  float64       \n",
            " 3   timestamp   1000000 non-null  datetime64[us]\n",
            "dtypes: datetime64[us](1), float64(1), int64(1), str(1)\n",
            "memory usage: 35.1 MB\n",
            "\n",
            "--- Performance Test: Summing Revenue ---\n",
            "Total revenue = $2,486,083.23\n",
            "Query executed in: 0.0120 seconds\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 2.2 Measure Memory Footprint and Query Time\n",
        "# ===============================================================\n",
        "\n",
        "print(\"--- Memory Usage ---\")\n",
        "\n",
        "# Display DataFrame info including RAM usage.\n",
        "# memory_usage='deep' forces Pandas to compute the REAL memory size,\n",
        "# including object overhead (not just raw array data).\n",
        "df.info(memory_usage=\"deep\")\n",
        "\n",
        "print(\"\\n--- Performance Test: Summing Revenue ---\")\n",
        "\n",
        "# Start timer to measure execution time of a simple aggregation\n",
        "start = time.time()\n",
        "\n",
        "# Filter only rows where the user actually purchased an item\n",
        "# Then sum the 'amount' column to compute total revenue.\n",
        "# This simulates a basic analytical query (GROUP BY-like operation).\n",
        "total_revenue = df[df[\"event_type\"] == \"purchase\"][\"amount\"].sum()\n",
        "\n",
        "# Compute total query duration\n",
        "duration = time.time() - start\n",
        "\n",
        "# Display results in a readable format\n",
        "print(f\"Total revenue = ${total_revenue:,.2f}\")\n",
        "print(f\"Query executed in: {duration:.4f} seconds\")\n",
        "\n",
        "# At this stage we learn two important Big Data concepts:\n",
        "# 1. VOLUME ‚Üí Even simple queries cost time when datasets grow.\n",
        "# 2. MEMORY ‚Üí DataFrames consume significant RAM, limiting scale on single machines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Concluding Remarks (Volume)\n",
        "Even with just 1 million rows, the dataset takes up measureable RAM (approx 30MB+) and takes a fraction of a second to query. \n",
        "\n",
        "In a real Big Data scenario (e.g., Facebook or Google), datasets are **petabytes** in size. A single machine with 16GB RAM would crash instantly trying to load it. This motivates the need for **Distributed Computing** (Hadoop/Spark), where we split this data across 1,000 machines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Variety: Handling Different Data Structures\n",
        "\n",
        "### Objective\n",
        "Data rarely comes in clean Excel spreadsheets. It often comes as **Semi-structured** logs (JSON) or **Unstructured** text or images.\n",
        "\n",
        "### What to do\n",
        "1. Read a traditional **CSV** (Structured).\n",
        "2. Read a nested **JSON** file (Semi-structured) simulating IoT/Server logs.\n",
        "3. Process simple **Text** lines (Unstructured)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved CSV file: transactions.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>event_type</th>\n",
              "      <th>amount</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15796</td>\n",
              "      <td>view</td>\n",
              "      <td>44.07</td>\n",
              "      <td>2025-01-01 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>861</td>\n",
              "      <td>click</td>\n",
              "      <td>11.96</td>\n",
              "      <td>2025-01-01 00:00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id event_type  amount            timestamp\n",
              "0    15796       view   44.07  2025-01-01 00:00:00\n",
              "1      861      click   11.96  2025-01-01 00:00:01"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 3.1 Structured Data: CSV (Comma-Separated Values)\n",
        "# ===============================================================\n",
        "# CSV is an example of a **structured** file format.\n",
        "# - It has a fixed schema (defined by column headers)\n",
        "# - Data is stored in rows and columns\n",
        "# - Easy to read with Pandas, Excel, SQL, etc.\n",
        "\n",
        "csv_path = Path(\"transactions.csv\")\n",
        "\n",
        "# Save our DataFrame to a CSV file on disk\n",
        "# 'index=False' avoids writing the DataFrame index as a column\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"Saved CSV file: {csv_path}\")\n",
        "\n",
        "# Reading CSV back into a DataFrame is straightforward\n",
        "# Pandas infers data types and loads it into memory\n",
        "csv_df = pd.read_csv(csv_path)\n",
        "\n",
        "# Show the first 2 rows as a quick validation check\n",
        "csv_df.head(2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsed Nested JSON:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>device</th>\n",
              "      <th>location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>mobile</td>\n",
              "      <td>{'country': 'SA', 'city': 'Riyadh'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>web</td>\n",
              "      <td>{'country': 'SA', 'city': 'Jeddah'}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>tablet</td>\n",
              "      <td>{'country': 'AE', 'city': 'Dubai'}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id  device                             location\n",
              "0        1  mobile  {'country': 'SA', 'city': 'Riyadh'}\n",
              "1        2     web  {'country': 'SA', 'city': 'Jeddah'}\n",
              "2        3  tablet   {'country': 'AE', 'city': 'Dubai'}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 3.2 Semi-Structured Data: JSON (JavaScript Object Notation)\n",
        "# ===============================================================\n",
        "# JSON is a **semi-structured** format commonly used in APIs, logs, and NoSQL systems.\n",
        "# Key characteristics:\n",
        "# - Supports hierarchical (nested) fields\n",
        "# - Keys can vary between records (flexible schema)\n",
        "# - Not restricted to rows/columns like CSV\n",
        "\n",
        "sample_events = [\n",
        "    {\"user_id\": 1, \"device\": \"mobile\", \"location\": {\"country\": \"SA\", \"city\": \"Riyadh\"}},\n",
        "    {\"user_id\": 2, \"device\": \"web\", \"location\": {\"country\": \"SA\", \"city\": \"Jeddah\"}},\n",
        "    {\"user_id\": 3, \"device\": \"tablet\", \"location\": {\"country\": \"AE\", \"city\": \"Dubai\"}},\n",
        "]\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Write JSON events to disk\n",
        "# ---------------------------------------------------------------\n",
        "# We write one JSON object per line (JSON Lines format), \n",
        "# which is commonly used for logs and streaming systems.\n",
        "with open(\"events.json\", \"w\") as f:\n",
        "    for event in sample_events:\n",
        "        f.write(json.dumps(event) + \"\\n\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Read JSON with Schema-on-Read\n",
        "# ---------------------------------------------------------------\n",
        "# Pandas infers structure when reading the file.\n",
        "# Notice how nested objects remain nested instead of flattened.\n",
        "\n",
        "json_df = pd.read_json(\"events.json\", lines=True)\n",
        "print(\"Parsed Nested JSON:\")\n",
        "json_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why is `location` Unflattened? (Schema-on-Read Explanation)**\n",
        "\n",
        "The `location` column remains unflattened because JSON is **semi-structured**, meaning nested fields and flexible keys are allowed. When we load the file with `pd.read_json(..., lines=True)`, Pandas applies **schema-on-read**: it infers structure at read time but does not automatically normalize nested objects into separate columns. Instead, it preserves the nested dictionary as a single cell so no information is lost. In real data systems (e.g., Spark, MongoDB, Data Lakes), schema-on-read allows applications to decide **later** how to interpret or flatten nested data depending on the query, making it flexible for evolving schemas and heterogeneous data sources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 3 lines of text.\n",
            "[\"User 1: 'Service is slow but accurate'\", \"User 2: 'Great price, fast delivery'\"]\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 3.3 Unstructured Data: Free Text (Logs / Reviews / Messages)\n",
        "# ===============================================================\n",
        "# Unstructured data has **no predefined schema**:\n",
        "# - No rows/columns\n",
        "# - No fixed types\n",
        "# - Meaning must be extracted manually (e.g., NLP, regex, ML)\n",
        "\n",
        "raw_text = \"\"\"\n",
        "User 1: 'Service is slow but accurate'\n",
        "User 2: 'Great price, fast delivery'\n",
        "User 3: 'App keeps crashing, bad experience'\n",
        "\"\"\"\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Convert raw multi-line text into a list of cleaned lines\n",
        "# ---------------------------------------------------------------\n",
        "# Steps:\n",
        "# 1. Split by newline characters\n",
        "# 2. Strip whitespace\n",
        "# 3. Filter out empty lines\n",
        "lines = [l for l in raw_text.split(\"\\n\") if l.strip()]\n",
        "\n",
        "print(f\"Processed {len(lines)} lines of text.\")\n",
        "print(lines[:2])  # Show first two lines as an example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Concluding Remarks (Variety)\n",
        "While CSVs are easy, modern Big Data systems deal heavily with JSON (APIs, MongoDB) and Unstructured data (reviews, images). Tools like **Spark** allow us to flatten nested JSON structures automatically, treating them like tables for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Veracity: The \"Dirty Data\" Reality\n",
        "\n",
        "### Objective\n",
        "Data in the real world is never clean. It has missing values (nulls), duplicates, and wrong formats. **Veracity** refers to the trustworthiness of data.\n",
        "\n",
        "### What to do\n",
        "1. Inject artificial errors into our dataset.\n",
        "2. Perform a **Data Quality Check** to find the errors.\n",
        "3. Execute a basic **Cleaning** step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating dirty dataset... done.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>event_type</th>\n",
              "      <th>amount</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>300266</th>\n",
              "      <td>35056</td>\n",
              "      <td>click</td>\n",
              "      <td>91.35</td>\n",
              "      <td>2025-01-04 11:24:26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308060</th>\n",
              "      <td>49570</td>\n",
              "      <td>click</td>\n",
              "      <td>42.09</td>\n",
              "      <td>2025-01-04 13:34:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>644152</th>\n",
              "      <td>66034</td>\n",
              "      <td>click</td>\n",
              "      <td>49.80</td>\n",
              "      <td>2025-01-08 10:55:52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>657624</th>\n",
              "      <td>45226</td>\n",
              "      <td>view</td>\n",
              "      <td>17.63</td>\n",
              "      <td>2025-01-08 14:40:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695424</th>\n",
              "      <td>32999</td>\n",
              "      <td>click</td>\n",
              "      <td>7.63</td>\n",
              "      <td>2025-01-09 01:10:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>706523</th>\n",
              "      <td>16619</td>\n",
              "      <td>purchase</td>\n",
              "      <td>5.20</td>\n",
              "      <td>2025-01-09 04:15:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41681</th>\n",
              "      <td>39300</td>\n",
              "      <td>view</td>\n",
              "      <td>11.11</td>\n",
              "      <td>2025-01-01 11:34:41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>459891</th>\n",
              "      <td>46132</td>\n",
              "      <td>UNKNOWN</td>\n",
              "      <td>61.79</td>\n",
              "      <td>2025-01-06 07:44:51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349608</th>\n",
              "      <td>69970</td>\n",
              "      <td>view</td>\n",
              "      <td>141.31</td>\n",
              "      <td>2025-01-05 01:06:48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5119</th>\n",
              "      <td>62523</td>\n",
              "      <td>click</td>\n",
              "      <td>33.10</td>\n",
              "      <td>2025-01-01 01:25:19</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        user_id event_type  amount           timestamp\n",
              "300266    35056      click   91.35 2025-01-04 11:24:26\n",
              "308060    49570      click   42.09 2025-01-04 13:34:20\n",
              "644152    66034      click   49.80 2025-01-08 10:55:52\n",
              "657624    45226       view   17.63 2025-01-08 14:40:24\n",
              "695424    32999      click    7.63 2025-01-09 01:10:24\n",
              "706523    16619   purchase    5.20 2025-01-09 04:15:23\n",
              "41681     39300       view   11.11 2025-01-01 11:34:41\n",
              "459891    46132    UNKNOWN   61.79 2025-01-06 07:44:51\n",
              "349608    69970       view  141.31 2025-01-05 01:06:48\n",
              "5119      62523      click   33.10 2025-01-01 01:25:19"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 4.1 Create Dirty Data (Simulating Real-World Data Issues)\n",
        "# ===============================================================\n",
        "\n",
        "# Take a random sample of 10,000 rows from the clean dataset\n",
        "# We use .copy() to avoid modifying the original DataFrame\n",
        "dirty_df = df.sample(10_000).copy()\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Inject Artificial Data Quality Problems\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# 1. Introduce missing values (NaN) in the 'amount' column\n",
        "#    This simulates cases where payment amounts were not recorded.\n",
        "dirty_df.loc[dirty_df.sample(500).index, \"amount\"] = np.nan\n",
        "\n",
        "# 2. Introduce corrupted / unexpected category values in 'event_type'\n",
        "#    This simulates data entry issues or unexpected API values.\n",
        "dirty_df.loc[dirty_df.sample(300).index, \"event_type\"] = \"UNKNOWN\"\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Result Preview\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "print(\"Creating dirty dataset... done.\")\n",
        "dirty_df.head(10)\n",
        "\n",
        "# At this point, dirty_df contains:\n",
        "# - Missing numeric values\n",
        "# - Invalid event categories\n",
        "# This prepares us for the 'Veracity' step (data cleaning).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Data Quality Report ---\n",
            "Missing Amounts: 500\n",
            "Invalid Events: 300\n",
            "\n",
            "--- Cleaning Complete ---\n",
            "Original Row Count: 10000\n",
            "Cleaned Row Count:  9700\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 4.2 Quality Check & Cleaning\n",
        "# ===============================================================\n",
        "\n",
        "print(\"--- Data Quality Report ---\")\n",
        "\n",
        "# Count how many rows have missing revenue (amount = NaN)\n",
        "print(f\"Missing Amounts: {dirty_df['amount'].isna().sum()}\")\n",
        "\n",
        "# Count how many rows contain invalid/corrupted event values\n",
        "print(f\"Invalid Events: {dirty_df[dirty_df['event_type'] == 'UNKNOWN'].shape[0]}\")\n",
        "\n",
        "# ===============================================================\n",
        "# CLEANING PIPELINE\n",
        "# ===============================================================\n",
        "\n",
        "# Always work on a copy to avoid changing original data unintentionally\n",
        "clean_df = dirty_df.copy()\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Step 1: Imputation\n",
        "# ---------------------------------------------------------------\n",
        "# Replace missing amounts with 0 (simple strategy for demonstration)\n",
        "# In real pipelines we might use mean/median imputation or ML models.\n",
        "clean_df[\"amount\"] = clean_df[\"amount\"].fillna(0)\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Step 2: Filtering\n",
        "# ---------------------------------------------------------------\n",
        "# Keep only event values that we consider valid\n",
        "# All rows with unexpected or corrupted event values are removed.\n",
        "valid_events = [\"click\", \"view\", \"purchase\"]\n",
        "clean_df = clean_df[clean_df[\"event_type\"].isin(valid_events)]\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Summary of Cleaning\n",
        "# ---------------------------------------------------------------\n",
        "print(\"\\n--- Cleaning Complete ---\")\n",
        "print(f\"Original Row Count: {len(dirty_df)}\")\n",
        "print(f\"Cleaned Row Count:  {len(clean_df)}\")\n",
        "\n",
        "# At this point:\n",
        "# - All missing amounts have been replaced with 0\n",
        "# - All invalid event types have been removed\n",
        "# The cleaned dataset is now safe for analysis or ML.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Concluding Remarks (Veracity)\n",
        "Before any analysis or Machine Learning can happen, data must be trusted. \"Garbage In, Garbage Out\" (GIGO) is the golden rule. Data Engineers spend a significant amount of time building these cleaning pipelines to ensure **Veracity**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. ETL vs ELT: Workflow Styles\n",
        "\n",
        "### Objective\n",
        "Understand the architectural difference between **ETL** (Extract, Transform, Load) and **ELT** (Extract, Load, Transform).\n",
        "\n",
        "### What to do\n",
        "1. **ETL Simulation**: We aggregate data *before* printing the final result. The raw details are discarded.\n",
        "2. **ELT Simulation**: We \"Load\" (save) the raw data first. Then we transform it later on demand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ETL Result (Stored in Warehouse):\n",
            "  country  amount\n",
            "0      AE      70\n",
            "1      SA     180\n",
            "\n",
            "ELT Result (Computed on demand):\n",
            "  country  amount\n",
            "0      AE      70\n",
            "1      SA     180\n"
          ]
        }
      ],
      "source": [
        "# user | amount | country\n",
        "# A    | 100    | SA\n",
        "# B    | 50     | SA\n",
        "# A    | 70     | AE\n",
        "# C    | 30     | SA\n",
        "\n",
        "# ===============================================================\n",
        "# Source Data (Raw)\n",
        "# ===============================================================\n",
        "# This represents transactional data BEFORE any processing.\n",
        "# Each row is one transaction (user, amount, country).\n",
        "raw_data = pd.DataFrame({\n",
        "    \"user\": [\"A\", \"B\", \"A\", \"C\"],\n",
        "    \"amount\": [100, 50, 70, 30],\n",
        "    \"country\": [\"SA\", \"SA\", \"AE\", \"SA\"]\n",
        "})\n",
        "\n",
        "# ===============================================================\n",
        "# 1. ETL Approach (Extract ‚Üí Transform ‚Üí Load)\n",
        "# ===============================================================\n",
        "# In ETL, we TRANSFORM the data BEFORE storing it.\n",
        "# Here, aggregation summarizes total amount per country\n",
        "# and only the SUMMARY results get saved/kept in the warehouse.\n",
        "etl_aggregate = raw_data.groupby(\"country\", as_index=False)[\"amount\"].sum()\n",
        "print(\"ETL Result (Stored in Warehouse):\")\n",
        "print(etl_aggregate)\n",
        "\n",
        "# IMPORTANT:\n",
        "# We lost row-level detail ‚Äî for example:\n",
        "# - User A had two separate transactions (100 and 70)\n",
        "# - Country SA had three separate transactions (100, 50, 30)\n",
        "# After aggregation, these multiple rows are merged into one,\n",
        "# so individual transaction details cannot be recovered.\n",
        "# That data CANNOT be recovered from the summary.\n",
        "# This is what we mean by \"loss of granularity\" in ETL.\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 2. ELT Approach (Extract ‚Üí Load ‚Üí Transform)\n",
        "# ===============================================================\n",
        "# STEP 1: LOAD raw data \"as-is\" into storage (Data Lake).\n",
        "# No transformation yet, so no data is lost.\n",
        "raw_data.to_csv(\"raw_data_lake.csv\", index=False)\n",
        "\n",
        "# STEP 2: TRANSFORM later, on demand.\n",
        "# Analysts or jobs can read the raw file and choose how to aggregate it.\n",
        "lake_df = pd.read_csv(\"raw_data_lake.csv\")\n",
        "elt_aggregate = lake_df.groupby(\"country\", as_index=False)[\"amount\"].sum()\n",
        "\n",
        "print(\"\\nELT Result (Computed on demand):\")\n",
        "print(elt_aggregate)\n",
        "\n",
        "# KEY DIFFERENCE:\n",
        "# Raw data still exists, so we could ask new questions later\n",
        "# (e.g., per-user stats, transaction counts), which is impossible with ETL.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Concluding Remarks (ETL vs ELT)\n",
        "In **ETL**, transformations happen early. This saves storage but loses granularity.  \n",
        "In **ELT** (Modern Data Lakes), we store *everything* first. This allows us to go back later and ask different questions (e.g., \"What was User A's specific timestamp?\") that would have been impossible with the ETL aggregate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. File Formats: CSV vs Parquet\n",
        "\n",
        "### Objective\n",
        "Big Data systems rarely use CSV for processing. They use binary columnar formats like **Parquet** or **ORC**. We will demonstrate why.\n",
        "\n",
        "### What to do\n",
        "1. Save our large dataset as both **CSV** and **Parquet**.\n",
        "2. Compare the **File Size** (Compression).\n",
        "3. Compare the **Read Speed** (Performance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving CSV... (this might take a moment)\n",
            "Saving Parquet...\n",
            "\n",
            "CSV Size:     16.73 MB\n",
            "Parquet Size: 4.61 MB\n",
            "Compression:  Parquet is 3.6x smaller!\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 6.1 Save Files in Different Formats (CSV vs Parquet)\n",
        "# ===============================================================\n",
        "# We extract only the columns needed for benchmarking storage formats.\n",
        "# NOTE: Smaller subset = faster saves for demonstration purposes.\n",
        "subset = df[[\"user_id\", \"event_type\", \"amount\"]].copy()\n",
        "\n",
        "# Define output file paths\n",
        "csv_path = Path(\"events.csv\")\n",
        "parquet_path = Path(\"events.parquet\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Save as CSV (Row-Based Format)\n",
        "# ---------------------------------------------------------------\n",
        "# CSV writes plain text, row by row.\n",
        "# This format does NOT apply compression or type optimization.\n",
        "print(\"Saving CSV... (this might take a moment)\")\n",
        "subset.to_csv(csv_path, index=False)\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Save as Parquet (Columnar Format)\n",
        "# ---------------------------------------------------------------\n",
        "# Parquet is a binary, columnar format optimized for analytics.\n",
        "# It applies compression under the hood (e.g., Snappy).\n",
        "print(\"Saving Parquet...\")\n",
        "subset.to_parquet(parquet_path, index=False)\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Compare File Sizes\n",
        "# ---------------------------------------------------------------\n",
        "# Convert bytes ‚Üí megabytes for human readability.\n",
        "csv_size = csv_path.stat().st_size / (1024 * 1024)\n",
        "pq_size = parquet_path.stat().st_size / (1024 * 1024)\n",
        "\n",
        "print(f\"\\nCSV Size:     {csv_size:.2f} MB\")\n",
        "print(f\"Parquet Size: {pq_size:.2f} MB\")\n",
        "print(f\"Compression:  Parquet is {csv_size/pq_size:.1f}x smaller!\")\n",
        "\n",
        "# This demonstrates:\n",
        "# - CSV = simple, readable, but storage-heavy\n",
        "# - Parquet = compact, binary, columnar, analytics-friendly\n",
        "# Parquet's efficiency becomes critical at Big Data scale.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Benchmarking Read Speed (Average of 3 runs) ---\n",
            "Reading CSV (Row-based)...\n",
            "128 ms ¬± 4.54 ms per loop (mean ¬± std. dev. of 3 runs, 3 loops each)\n",
            "\n",
            "Reading Parquet (Columnar-based)...\n",
            "The slowest run took 58.69 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "298 ms ¬± 401 ms per loop (mean ¬± std. dev. of 3 runs, 3 loops each)\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# 6.2 Benchmark Read Speed (CSV vs Parquet)\n",
        "# ===============================================================\n",
        "# We measure how long it takes to:\n",
        "# 1. Read the file from disk\n",
        "# 2. Select the `amount` column\n",
        "# 3. Compute the mean\n",
        "#\n",
        "# The %timeit magic command runs multiple repetitions to get\n",
        "# a stable average runtime, reducing noise caused by the system.\n",
        "\n",
        "print(\"\\n--- Benchmarking Read Speed (Average of 3 runs) ---\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Benchmark: CSV (Row-Based Format)\n",
        "# ---------------------------------------------------------------\n",
        "# CSV must parse the file line-by-line, converting text ‚Üí numbers.\n",
        "# This is slower because we read ALL columns and ALL rows as text.\n",
        "print(\"Reading CSV (Row-based)...\")\n",
        "%timeit -n3 -r3 pd.read_csv(csv_path)[\"amount\"].mean()\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Benchmark: Parquet (Columnar Format)\n",
        "# ---------------------------------------------------------------\n",
        "# Parquet stores data in binary, by columns, with schema included.\n",
        "# For analytical queries (like computing mean of one column),\n",
        "# Parquet can read just the needed column, making it more efficient.\n",
        "print(\"\\nReading Parquet (Columnar-based)...\")\n",
        "%timeit -n3 -r3 pd.read_parquet(parquet_path)[\"amount\"].mean()\n",
        "\n",
        "# NOTE:\n",
        "# For small files and local machines, differences may vary\n",
        "# because Parquet has some overhead (metadata + decompression).\n",
        "# At large scale (GB‚ÄìTB), columnar formats are much faster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Concluding Remarks (Formats)\n",
        "**Parquet is significantly faster and smaller.**\n",
        "\n",
        "*   **Size**: Parquet uses **smart compression** (like run-length encoding), making it much cheaper to store.\n",
        "*   **Speed**: Because it is **Columnar**, calculating the average `amount` only requires reading that one column. The CSV reader must parse every single row text-by-line, which is slow.\n",
        "\n",
        "Use **Columnar formats** (Parquet/ORC) for Big Data analytics!\n",
        "\n",
        "Here‚Äôs a clear **~150-word educational explanation** of the results:\n",
        "\n",
        "---\n",
        "### Concluding Remarks (Formats)\n",
        "This experiment compares CSV and Parquet as data storage formats. When we saved the same subset of data, the CSV file took about **16.7 MB**, while the Parquet file took only **4.6 MB**. This means Parquet was roughly **3.6√ó smaller**. The reason is that Parquet uses **columnar storage** and **built-in compression**, while CSV stores everything as plain text with no compression.\n",
        "\n",
        "Then we benchmarked read performance. Reading the CSV took around **128 ms**, while Parquet showed a slower and more variable time (around **298 ms ¬± large variation**). This may seem surprising because Parquet is often faster on big analytics workloads, but here we only read a small file on a single machine. In this situation, the overhead of decompression and metadata handling can outweigh the benefits.\n",
        "\n",
        "So the takeaway is: **CSV is simple and good for small data**, while **Parquet is space-efficient and optimized for large-scale analytics, Spark, and columnar queries**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Pandas vs Spark DataFrames: Why Big Data Needs Distributed DataFrames\n",
        "\n",
        "### Objective\n",
        "**Learning Objectives:**\n",
        "1. **Execution Model**: Understand *where* your code runs (Single CPU vs Cluster).\n",
        "2. **Overhead vs. Scale**: See why Spark is slower for small data but necessary for big data.\n",
        "3. **Lazy Evaluation**: Learn that Spark (unlike Pandas) doesn't compute until you ask for results (like `.show()`).\n",
        "\n",
        "### Understanding the Complexity: Why is Spark Harder to Run?\n",
        "You might wonder: *\"Why do we need Java? Why all these environment variables? Why do we need to restart the runtime?\"*\n",
        "\n",
        "Great question! Spark is complex to run because of its **distributed computing architecture**. Here's the simple explanation:\n",
        "\n",
        "#### 1. Spark vs Regular Python\n",
        "\n",
        "**Regular Python (Pandas)**\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   Your Computer     ‚îÇ\n",
        "‚îÇ   Python ‚Üí CPU      ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "      Simple!\n",
        "```\n",
        "\n",
        "**Spark**\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Your Code (Python)                         ‚îÇ\n",
        "‚îÇ       ‚Üì                                     ‚îÇ\n",
        "‚îÇ  PySpark (Python library)                   ‚îÇ\n",
        "‚îÇ       ‚Üì                                     ‚îÇ\n",
        "‚îÇ  Py4J (Python-to-Java bridge)               ‚îÇ\n",
        "‚îÇ       ‚Üì                                     ‚îÇ\n",
        "‚îÇ  Spark Core (Java/Scala - needs JVM)        ‚îÇ\n",
        "‚îÇ       ‚Üì                                     ‚îÇ\n",
        "‚îÇ  Cluster Manager (even \"local\" mode)        ‚îÇ\n",
        "‚îÇ       ‚Üì                                     ‚îÇ\n",
        "‚îÇ  Workers (parallel processing)              ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "      Many layers!\n",
        "```\n",
        "\n",
        "#### 2. Why So Many Dependencies?\n",
        "\n",
        "| Dependency | Why It's Needed |\n",
        "|------------|-----------------|\n",
        "| **Java (JVM)** | Spark is written in Scala, which runs on Java |\n",
        "| **Py4J** | Translates Python calls to Java |\n",
        "| **Hadoop libraries** | File system handling (HDFS, S3, etc.) |\n",
        "| **Environment variables** | So all pieces can find each other |\n",
        "\n",
        "#### 3. The Tradeoff\n",
        "\n",
        "| | Pandas | Spark |\n",
        "|--|--------|-------|\n",
        "| **Setup** | `pip install pandas` ‚úÖ | Java + PySpark + config üòÖ |\n",
        "| **Data size** | ~1-10 GB (RAM limit) | Petabytes across clusters |\n",
        "| **Speed (small data)** | **Faster** | Slower (overhead) |\n",
        "| **Speed (big data)** | Crashes üí• | **Handles it easily** ‚úÖ |\n",
        "\n",
        "#### ‚ö†Ô∏è IMPORTANT: Run the Installation Cell first, then RESTART RUNTIME!\n",
        "Colab requires a runtime restart after changing environment variables (Java). Go to **Runtime -> Restart Session** after running Step 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/25 21:55:22 WARN Utils: Your hostname, Anis-Koubaas-MacBook-Pro-10.local resolves to a loopback address: 127.0.0.1; using 192.168.1.197 instead (on interface en0)\n",
            "26/01/25 21:55:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "26/01/25 21:55:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Spark Version: 3.5.0\n",
            "‚úÖ Spark Session Created!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----+\n",
            "| id|value|\n",
            "+---+-----+\n",
            "|  1|hello|\n",
            "|  2|spark|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# STEP 1: Initialize Spark Session\n",
        "# ===============================================================\n",
        "# IMPORTANT: In Colab, you must restart the runtime after installing Java + PySpark\n",
        "# before running this cell, otherwise Spark will fail to start.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import platform\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Configure JAVA_HOME (Spark runs on the JVM)\n",
        "# ---------------------------------------------------------------\n",
        "# We detect the environment and set the correct Java path.\n",
        "# - Google Colab uses system OpenJDK\n",
        "# - macOS uses Homebrew path (if installed that way)\n",
        "# - Linux and Windows users may need manual adjustments\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "else:\n",
        "    if platform.system() == \"Darwin\":  # macOS\n",
        "        os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@11/libexec/openjdk.jdk/Contents/Home\"\n",
        "    elif platform.system() == \"Linux\":  # Linux Desktop\n",
        "        os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "    # Windows users: set JAVA_HOME manually if needed\n",
        "\n",
        "# Tell PySpark to use the current Python interpreter\n",
        "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Create Spark Session\n",
        "# ---------------------------------------------------------------\n",
        "# .master(\"local[*]\") means:\n",
        "# - Run Spark locally\n",
        "# - Use as many CPU cores as available\n",
        "#\n",
        "# Even though this is \"local mode\", Spark still uses the full\n",
        "# distributed execution engine architecture under the hood.\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BigDataDemo\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"‚úÖ Spark Version: {spark.version}\")\n",
        "print(\"‚úÖ Spark Session Created!\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Quick Test DataFrame\n",
        "# ---------------------------------------------------------------\n",
        "# If the following prints a small table, Spark is working correctly.\n",
        "df = spark.createDataFrame([(1, \"hello\"), (2, \"spark\")], [\"id\", \"value\"])\n",
        "df.show()\n",
        "\n",
        "# At this point:\n",
        "# - Spark is running on your machine\n",
        "# - PySpark is bridging Python ‚Üî JVM\n",
        "# - Ready for distributed DataFrame operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Spark Version: 3.5.0\n",
            "‚úÖ Spark Session Created!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/25 21:55:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+-----+\n",
            "| id|value|\n",
            "+---+-----+\n",
            "|  1| test|\n",
            "|  2|spark|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# STEP 2: Initialize Spark Session\n",
        "# ===============================================================\n",
        "# NOTE: In Colab, Spark requires Java. After installing Java/JDK\n",
        "# and setting environment variables in the previous step,\n",
        "# you MUST restart the runtime before running this cell.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Re-set environment variables after restart\n",
        "# ---------------------------------------------------------------\n",
        "# JAVA_HOME tells PySpark where to find the JVM (Java Virtual Machine)\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "# Add Java binaries to PATH so the 'java' command is available\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Create SparkSession (entry point to Spark functionality)\n",
        "# ---------------------------------------------------------------\n",
        "# Explanation:\n",
        "# - .appName(\"BigDataDemo\") names the application\n",
        "# - .master(\"local[*]\") runs Spark in local mode\n",
        "#      using all available CPU cores\n",
        "# - .config(\"spark.driver.memory\", \"4g\") allocates memory to driver\n",
        "#\n",
        "# Even in \"local\" mode, Spark uses its distributed execution engine.\n",
        "try:\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"BigDataDemo\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.driver.memory\", \"4g\") \\\n",
        "        .getOrCreate()\n",
        "    \n",
        "    print(f\"‚úÖ Spark Version: {spark.version}\")\n",
        "    print(\"‚úÖ Spark Session Created!\")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Common error handling\n",
        "# ---------------------------------------------------------------\n",
        "# If Java is not configured correctly, PySpark may throw:\n",
        "# 'JavaPackage object is not callable'\n",
        "# This indicates that JAVA_HOME wasn't recognized at startup.\n",
        "except TypeError as e:\n",
        "    print(\"\\n‚ùå ERROR: Spark could not connect to Java.\")\n",
        "    print(\"If you see 'JavaPackage object is not callable', restart the runtime, then re-run this cell.\")\n",
        "    print(\"Menu: Runtime ‚Üí Restart Session\")\n",
        "    raise e\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Quick sanity check: create a tiny DataFrame\n",
        "# ---------------------------------------------------------------\n",
        "# If this prints as a table, Spark is working correctly.\n",
        "df_check = spark.createDataFrame([(1, \"test\"), (2, \"spark\")], [\"id\", \"value\"])\n",
        "df_check.show()\n",
        "\n",
        "# At this point Spark is ready for:\n",
        "# - distributed DataFrame operations\n",
        "# - reading Parquet/JSON/CSV in distributed mode\n",
        "# - running SQL queries via spark.sql()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Step 3: Create DataFrames (Pandas vs Spark)\n",
        "\n",
        "### Creating DataFrames: The \"Hello World\" of Spark\n",
        "\n",
        "Now that Spark is running, let's create some data to compare.\n",
        "1.  **Pandas**: We create a DataFrame in **local memory**. Easy and fast for this size.\n",
        "2.  **Spark**: We convert that Pandas DF into a **Distributed DataFrame**. \n",
        "    *   *Note*: In a real scenario, you would read from S3/HDFS directly. Converting local Pandas -> Spark varies in speed but is good for demos.\n",
        "    \n",
        "This simulates \"Ingestion\".\n",
        "\n",
        "## What's Happening Here?\n",
        "\n",
        "1. **Create data with Pandas** ‚Üí Lives in your computer's RAM (single machine)\n",
        "2. **Convert to Spark DataFrame** ‚Üí Data is now distributed and parallelized\n",
        "\n",
        "## Key Difference\n",
        "\n",
        "| Pandas DataFrame | Spark DataFrame |\n",
        "|------------------|-----------------|\n",
        "| Stored in RAM | Distributed across workers |\n",
        "| Processed by 1 CPU | Processed by many CPUs in parallel |\n",
        "| `pdf` (Python object) | `sdf` (pointer to distributed data) |\n",
        "\n",
        "## Why Convert?\n",
        "\n",
        "Pandas is great for creating/loading data, but Spark can **process it in parallel** ‚Äî essential when data grows to billions of rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 1,000,000 rows in Pandas...\n",
            "Converting to Spark DataFrame...\n",
            "Done.\n",
            "+-------+------+--------+\n",
            "|user_id|amount|   event|\n",
            "+-------+------+--------+\n",
            "|  95042| 84.94|   click|\n",
            "|  97917| 56.52|purchase|\n",
            "|  50646| 22.82|    view|\n",
            "|  86286| 20.84|   click|\n",
            "|  66212| 55.16|purchase|\n",
            "+-------+------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/25 21:55:39 WARN TaskSetManager: Stage 6 contains a task of very large size (1902 KiB). The maximum recommended task size is 1000 KiB.\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# STEP 3: Create DataFrames (Pandas vs Spark)\n",
        "# ===============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# A. Generate Data in Pandas (In-Memory)\n",
        "# ---------------------------------------------------------------\n",
        "# We create 1,000,000 rows directly in RAM using Pandas.\n",
        "# Pandas DataFrames live on a single machine and use a single CPU.\n",
        "N = 1_000_000\n",
        "print(f\"Generating {N:,} rows in Pandas...\")\n",
        "\n",
        "pdf = pd.DataFrame({\n",
        "    \"user_id\": np.random.randint(1, 100_000, size=N),  # Many users, repeated IDs\n",
        "    \"amount\": np.round(np.random.exponential(scale=50, size=N), 2),  # Skewed monetary values\n",
        "    \"event\": np.random.choice([\"click\", \"view\", \"purchase\"], size=N)  # Categorical events\n",
        "})\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# B. Convert Pandas DataFrame to Spark DataFrame (Distributed)\n",
        "# ---------------------------------------------------------------\n",
        "# Converting to a Spark DataFrame allows distributed operations.\n",
        "# In a real use case, Spark would read from files (S3/HDFS/Parquet).\n",
        "print(\"Converting to Spark DataFrame...\")\n",
        "\n",
        "sdf = spark.createDataFrame(pdf)\n",
        "\n",
        "print(\"Done.\")\n",
        "\n",
        "# Display the first 5 rows of the Spark DataFrame\n",
        "# This triggers a Spark action (lazy evaluation until .show\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Phase 3 Output ‚Äî Key Takeaways**\n",
        "\n",
        "## **About the Warnings**\n",
        "\n",
        "You may see warnings such as:\n",
        "\n",
        "| Warning                                  | Meaning                                                                 |\n",
        "|------------------------------------------|-------------------------------------------------------------------------|\n",
        "| `Task of very large size (1901 KiB)`     | Spark prefers smaller partitions; we sent ~1M rows as one block.        |\n",
        "| `Detected deadlock`                      | Temporary stall between Python ‚Üî JVM; Spark recovers automatically.     |\n",
        "\n",
        "> **Note:** These warnings are common when running Spark locally with small datasets.\n",
        "> Spark is optimized for distributed clusters and massive data workloads.\n",
        "\n",
        "---\n",
        "\n",
        "# **Performance Difference (Pandas vs Spark)**\n",
        "\n",
        "When benchmarking with `%time`:\n",
        "\n",
        "### **What `%time` Reports**\n",
        "| Metric        | Meaning                               |\n",
        "|---------------|---------------------------------------|\n",
        "| **CPU time**  | Actual computation time                |\n",
        "| **Wall time** | Total real-world waiting time          |\n",
        "\n",
        "**Example:** CPU = 5 ms vs Wall = 600 ms ‚Üí most time spent waiting (setup/communication).\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Pandas Is Faster Here**\n",
        "\n",
        "**Pandas**\n",
        "- Data is already in your computer‚Äôs memory (RAM)\n",
        "- Runs directly on the CPU\n",
        "- No extra setup or communication steps  \n",
        "‚û°Ô∏è **So it's fast (usually milliseconds)**\n",
        "\n",
        "**Spark**\n",
        "1. Needs to create tasks\n",
        "2. Needs to organize workers (even if only 1 worker on your laptop)\n",
        "3. Needs to send data between **Python** and **Java (JVM)**\n",
        "4. Waits until an action is called (lazy evaluation)  \n",
        "‚û°Ô∏è **So there is a startup cost (usually seconds)**\n",
        "\n",
        "üëâ **Important:** Spark is not slower because it's bad ‚Äî Spark is designed for **huge datasets and clusters (+10M)**, not tiny demos on a laptop. For big data, Spark wins. For small data, Pandas wins.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Core Lesson**\n",
        "Spark has a **fixed startup cost**. For **small data**, overhead dominates and Spark feels slow.\n",
        "\n",
        "For **large data (GB‚ÄìTB)**, Spark‚Äôs parallelism becomes a major advantage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Pandas (Single-Core) ---\n",
            "CPU times: user 45 ms, sys: 25 ms, total: 70 ms\n",
            "Wall time: 82.4 ms\n",
            "\n",
            "--- Spark (Distributed Plan) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/25 21:58:58 WARN TaskSetManager: Stage 10 contains a task of very large size (1902 KiB). The maximum recommended task size is 1000 KiB.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+------------------+\n",
            "|   event|       avg(amount)|\n",
            "+--------+------------------+\n",
            "|purchase| 50.01232078702823|\n",
            "|    view|49.962862479696966|\n",
            "|   click|50.058101695830004|\n",
            "+--------+------------------+\n",
            "\n",
            "CPU times: user 2.57 ms, sys: 2.24 ms, total: 4.8 ms\n",
            "Wall time: 769 ms\n"
          ]
        }
      ],
      "source": [
        "# ===============================================================\n",
        "# STEP 4: Compare Computations (Pandas vs Spark)\n",
        "# ===============================================================\n",
        "\n",
        "print(\"--- Pandas (Single-Core) ---\")\n",
        "# ---------------------------------------------------------------\n",
        "# Pandas executes eagerly and in-memory on a single CPU core.\n",
        "# %time measures how long the groupby + mean operation takes.\n",
        "# ---------------------------------------------------------------\n",
        "%time pdf.groupby(\"event\")[\"amount\"].mean()\n",
        "\n",
        "print(\"\\n--- Spark (Distributed Plan) ---\")\n",
        "# ---------------------------------------------------------------\n",
        "# Spark uses lazy evaluation: it builds a logical execution plan\n",
        "# but does not actually run it until an ACTION is triggered.\n",
        "#\n",
        "# groupBy(...).avg(...) alone does nothing until .show(), .collect(),\n",
        "# .count(), or similar actions are called.\n",
        "#\n",
        "# %time measures the time to:\n",
        "#   1. plan the distributed job\n",
        "#   2. schedule tasks\n",
        "#   3. execute the aggregation\n",
        "#   4. collect results to the driver for printing\n",
        "# ---------------------------------------------------------------\n",
        "%time sdf.groupBy(\"event\").avg(\"amount\").show()  # .show() triggers execution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pandas completes the aggregation in ~80 ms because it runs eagerly in-memory on one core with no coordination overhead. Spark takes ~770 ms because it must plan, schedule, and coordinate a distributed job before executing. For small data, Spark‚Äôs startup overhead dominates; for large data, its parallelism becomes beneficial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° Concluding Remarks (Pandas vs Spark)\n",
        "\n",
        "*   **Pandas** is incredibly fast for data that fits in RAM (like this 1M row example), often beating Spark due to low overhead.\n",
        "*   **Spark** has overhead (starting tasks, communicating), so it might seem slower here.\n",
        "*   **HOWEVER**: If we had **1 Billion rows**, Pandas would crash with an `Out of Memory` error. Spark would simply split the data into chunks and process it in parallel, taking longer but completing successfully.\n",
        "\n",
        "**Rule of Thumb:** Use Pandas for MBs/GBs. Use Spark for TBs/PBs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. Short Reflection\n",
        "\n",
        "**Instructions:**\n",
        "Based on the exercises above, write a short reflection (3-5 sentences) answering the following:\n",
        "\n",
        "1. Which activity demonstrated the biggest performance difference?\n",
        "2. Why do you think \"Schema-on-Read\" (handling JSON) is important for modern apps versus traditional SQL tables?\n",
        "3. Why would a company prefer ELT (Data Lake) over ETL (Data Warehouse) today?\n",
        "4. Why does Spark feel slower than Pandas for small data, but is preferred for Big Data?\n",
        "\n",
        "*(Double-click here to write your answer)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
