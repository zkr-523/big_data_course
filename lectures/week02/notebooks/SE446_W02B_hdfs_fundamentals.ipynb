{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SE446 - Week 1B: HDFS Fundamentals\n",
                "\n",
                "**Course:** SE 446 - Big Data Analytics  \n",
                "**Instructor:** Prof. Anis Koubaa  \n",
                "**Session:** Week 1, Session B\n",
                "\n",
                "---\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this session, you will be able to:\n",
                "1. Explain HDFS architecture (NameNode, DataNode)\n",
                "2. Understand data replication and fault tolerance\n",
                "3. Compare file formats: CSV, JSON, Parquet\n",
                "4. Perform basic data loading in Python (simulating HDFS concepts)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. HDFS Architecture\n",
                "\n",
                "**HDFS** (Hadoop Distributed File System) stores files across multiple machines.\n",
                "\n",
                "### Key Components\n",
                "\n",
                "| Component | Role | Analogy |\n",
                "|-----------|------|--------|\n",
                "| **NameNode** | Master node, stores metadata | Library catalog |\n",
                "| **DataNode** | Worker nodes, store actual data | Library shelves |\n",
                "| **Block** | Fixed-size chunk of file (128 MB default) | Book chapters |\n",
                "\n",
                "### Architecture Diagram\n",
                "\n",
                "```\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚                        HDFS CLUSTER                         â”‚\n",
                "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
                "â”‚                                                             â”‚\n",
                "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\n",
                "â”‚  â”‚           NameNode                â”‚                     â”‚\n",
                "â”‚  â”‚  â€¢ File names and paths           â”‚                     â”‚\n",
                "â”‚  â”‚  â€¢ Block locations                â”‚                     â”‚\n",
                "â”‚  â”‚  â€¢ Replication info               â”‚                     â”‚\n",
                "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\n",
                "â”‚           â”‚           â”‚           â”‚                         â”‚\n",
                "â”‚           â–¼           â–¼           â–¼                         â”‚\n",
                "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
                "â”‚  â”‚DataNode 1â”‚  â”‚DataNode 2â”‚  â”‚DataNode 3â”‚                  â”‚\n",
                "â”‚  â”‚ Block A  â”‚  â”‚ Block A  â”‚  â”‚ Block B  â”‚                  â”‚\n",
                "â”‚  â”‚ Block B  â”‚  â”‚ Block C  â”‚  â”‚ Block A  â”‚  â† Replication   â”‚\n",
                "â”‚  â”‚ Block C  â”‚  â”‚ Block B  â”‚  â”‚ Block C  â”‚                  â”‚\n",
                "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
                "â”‚                                                             â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Task 1: HDFS Concepts Quiz (5 points)\n",
                "\n",
                "Answer the following questions about HDFS architecture."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TASK 1: Fill in the correct answers about HDFS\n",
                "\n",
                "hdfs_quiz = {\n",
                "    \"Which node stores metadata (file names, block locations)?\": \"_____\",  # NameNode or DataNode?\n",
                "    \"Which node stores the actual data blocks?\": \"_____\",                   # NameNode or DataNode?\n",
                "    \"Default block size in HDFS (in MB)?\": _____,                            # 64, 128, or 256?\n",
                "    \"Default replication factor?\": _____,                                    # 1, 2, or 3?\n",
                "    \"If NameNode fails, what happens to the cluster?\": \"_____\"              # 'continues' or 'stops'?\n",
                "}\n",
                "\n",
                "print(\"HDFS Architecture Quiz\")\n",
                "print(\"=\" * 50)\n",
                "for question, answer in hdfs_quiz.items():\n",
                "    print(f\"Q: {question}\")\n",
                "    print(f\"A: {answer}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Data Replication\n",
                "\n",
                "HDFS replicates each block across multiple DataNodes for **fault tolerance**.\n",
                "\n",
                "### Example: Replication Factor = 3\n",
                "\n",
                "```\n",
                "Original File: report.csv (500 MB)\n",
                "                    â”‚\n",
                "                    â–¼\n",
                "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "        â”‚     Split into        â”‚\n",
                "        â”‚    4 blocks (128 MB)  â”‚\n",
                "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "                    â”‚\n",
                "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
                "    â–¼       â–¼       â–¼       â–¼       â–¼\n",
                "Block 1  Block 2  Block 3  Block 4\n",
                "(128MB)  (128MB)  (128MB)  (116MB)\n",
                "    â”‚       â”‚       â”‚       â”‚\n",
                "    â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”\n",
                "                                    â”‚\n",
                "                          Each block copied\n",
                "                          to 3 DataNodes\n",
                "```\n",
                "\n",
                "**Total storage used:** 500 MB Ã— 3 = 1.5 GB"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Task 2: Calculate HDFS Storage (5 points)\n",
                "\n",
                "Calculate the actual storage required for files in HDFS."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TASK 2: Calculate HDFS storage requirements\n",
                "\n",
                "def calculate_hdfs_storage(file_size_mb, block_size_mb=128, replication_factor=3):\n",
                "    \"\"\"\n",
                "    Calculate total storage used in HDFS.\n",
                "    \n",
                "    Args:\n",
                "        file_size_mb: Size of the original file in MB\n",
                "        block_size_mb: Size of each block (default 128 MB)\n",
                "        replication_factor: Number of copies (default 3)\n",
                "    \n",
                "    Returns:\n",
                "        Tuple of (num_blocks, total_storage_mb)\n",
                "    \"\"\"\n",
                "    import math\n",
                "    \n",
                "    # TODO: Calculate number of blocks (round up!)\n",
                "    num_blocks = _____  # <-- FILL THIS: math.ceil(file_size_mb / block_size_mb)\n",
                "    \n",
                "    # TODO: Calculate total storage with replication\n",
                "    total_storage_mb = _____  # <-- FILL THIS: file_size_mb * replication_factor\n",
                "    \n",
                "    return num_blocks, total_storage_mb\n",
                "\n",
                "# Test with different files\n",
                "test_files = [\n",
                "    (\"small_log.txt\", 50),      # 50 MB file\n",
                "    (\"dataset.csv\", 500),        # 500 MB file\n",
                "    (\"video.mp4\", 2048),         # 2 GB file\n",
                "]\n",
                "\n",
                "print(\"HDFS Storage Calculator\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"{'File':<20} {'Size':<10} {'Blocks':<10} {'Total Storage':<15}\")\n",
                "print(\"-\" * 60)\n",
                "\n",
                "for filename, size_mb in test_files:\n",
                "    blocks, total = calculate_hdfs_storage(size_mb)\n",
                "    print(f\"{filename:<20} {size_mb} MB{'':<4} {blocks:<10} {total} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. File Formats Comparison\n",
                "\n",
                "Different file formats have different properties:\n",
                "\n",
                "| Format | Type | Compression | Schema | Best For |\n",
                "|--------|------|-------------|--------|----------|\n",
                "| **CSV** | Row-based | None | No | Simple data exchange |\n",
                "| **JSON** | Row-based | None | Self-describing | APIs, configs |\n",
                "| **Parquet** | Column-based | Yes | Embedded | Analytics, Big Data |\n",
                "\n",
                "### Why Parquet for Big Data?\n",
                "\n",
                "1. **Columnar storage**: Read only needed columns\n",
                "2. **Compression**: 10x smaller than CSV\n",
                "3. **Schema embedded**: No need for external schema files\n",
                "4. **Predicate pushdown**: Filter at storage level"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup: Install required libraries (run once)\n",
                "!pip install pandas pyarrow -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's create sample data and compare file formats\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "\n",
                "# Create sample data (10,000 rows)\n",
                "np.random.seed(42)\n",
                "n_rows = 10000\n",
                "\n",
                "data = {\n",
                "    'id': range(n_rows),\n",
                "    'name': [f'User_{i}' for i in range(n_rows)],\n",
                "    'age': np.random.randint(18, 65, n_rows),\n",
                "    'salary': np.random.randint(30000, 150000, n_rows),\n",
                "    'department': np.random.choice(['Engineering', 'Sales', 'Marketing', 'HR'], n_rows)\n",
                "}\n",
                "\n",
                "df = pd.DataFrame(data)\n",
                "print(\"Sample Data:\")\n",
                "print(df.head())\n",
                "print(f\"\\nTotal rows: {len(df):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Task 3: Compare File Formats (10 points)\n",
                "\n",
                "Save the data in different formats and compare their sizes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TASK 3: Save data in different formats and compare sizes\n",
                "\n",
                "# Save as CSV\n",
                "df.to_csv('sample_data.csv', index=False)\n",
                "csv_size = os.path.getsize('sample_data.csv')\n",
                "\n",
                "# Save as JSON\n",
                "df.to_json('sample_data.json', orient='records')\n",
                "json_size = os.path.getsize('sample_data.json')\n",
                "\n",
                "# TODO: Save as Parquet\n",
                "# HINT: Use df.to_parquet('filename.parquet')\n",
                "_____  # <-- FILL THIS: df.to_parquet('sample_data.parquet')\n",
                "parquet_size = os.path.getsize('sample_data.parquet')\n",
                "\n",
                "# Calculate compression ratios\n",
                "print(\"\\nFile Size Comparison\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"{'Format':<15} {'Size (KB)':<15} {'Ratio vs CSV':<15}\")\n",
                "print(\"-\" * 50)\n",
                "print(f\"{'CSV':<15} {csv_size/1024:.2f} KB{'':<6} 1.00x\")\n",
                "print(f\"{'JSON':<15} {json_size/1024:.2f} KB{'':<6} {json_size/csv_size:.2f}x\")\n",
                "print(f\"{'Parquet':<15} {parquet_size/1024:.2f} KB{'':<6} {parquet_size/csv_size:.2f}x\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Bonus: Read only specific columns from Parquet (columnar benefit!)\n",
                "\n",
                "# TODO: Read only 'name' and 'salary' columns from Parquet\n",
                "# HINT: Use pd.read_parquet('file', columns=['col1', 'col2'])\n",
                "\n",
                "df_subset = _____  # <-- FILL THIS: pd.read_parquet('sample_data.parquet', columns=['name', 'salary'])\n",
                "\n",
                "print(\"Reading only 'name' and 'salary' columns:\")\n",
                "print(df_subset.head())\n",
                "print(f\"\\nColumns loaded: {list(df_subset.columns)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Loading Real Data\n",
                "\n",
                "Let's load a sample of the NYC Taxi dataset - a classic Big Data example."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load NYC Taxi data sample\n",
                "# Note: This is a small sample for learning purposes\n",
                "\n",
                "url = \"https://raw.githubusercontent.com/plotly/datasets/master/uber-rides-data1.csv\"\n",
                "\n",
                "taxi_df = pd.read_csv(url)\n",
                "\n",
                "print(\"NYC Taxi/Uber Data Sample\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Shape: {taxi_df.shape[0]:,} rows Ã— {taxi_df.shape[1]} columns\")\n",
                "print(f\"\\nColumns: {list(taxi_df.columns)}\")\n",
                "print(f\"\\nFirst 5 rows:\")\n",
                "taxi_df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Task 4: Basic Data Exploration (5 points)\n",
                "\n",
                "Explore the taxi dataset using pandas."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TASK 4: Basic data exploration\n",
                "\n",
                "# TODO: Get the number of rows\n",
                "num_rows = _____  # <-- FILL THIS: len(taxi_df) or taxi_df.shape[0]\n",
                "\n",
                "# TODO: Get the number of columns\n",
                "num_cols = _____  # <-- FILL THIS: taxi_df.shape[1]\n",
                "\n",
                "# TODO: Get memory usage in MB\n",
                "memory_mb = _____  # <-- FILL THIS: taxi_df.memory_usage(deep=True).sum() / (1024*1024)\n",
                "\n",
                "print(\"Dataset Summary\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"Number of rows:     {num_rows:,}\")\n",
                "print(f\"Number of columns:  {num_cols}\")\n",
                "print(f\"Memory usage:       {memory_mb:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. HDFS Commands Reference\n",
                "\n",
                "In a real Hadoop cluster, you would use these commands:\n",
                "\n",
                "| Command | Description | Example |\n",
                "|---------|-------------|---------|\n",
                "| `hdfs dfs -ls /` | List files | `hdfs dfs -ls /user/data/` |\n",
                "| `hdfs dfs -put` | Upload file | `hdfs dfs -put local.csv /data/` |\n",
                "| `hdfs dfs -get` | Download file | `hdfs dfs -get /data/file.csv ./` |\n",
                "| `hdfs dfs -mkdir` | Create directory | `hdfs dfs -mkdir /user/me/` |\n",
                "| `hdfs dfs -rm` | Delete file | `hdfs dfs -rm /data/old.csv` |\n",
                "| `hdfs dfs -cat` | View file content | `hdfs dfs -cat /data/file.txt` |\n",
                "\n",
                "*Note: In this course, we simulate HDFS operations using Python/Colab. Real HDFS operations will be covered using Databricks in later weeks.*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ğŸ“ Reflection Questions (For ExamGPT)\n",
                "\n",
                "Answer these questions in the ExamGPT quiz at the end of class:\n",
                "\n",
                "**Q1:** What is the role of the NameNode in HDFS?\n",
                "\n",
                "**Q2:** If we store a 500 MB file with replication factor 3, how much total storage is used?\n",
                "\n",
                "**Q3:** Why is Parquet better than CSV for Big Data analytics? (Give 2 reasons)\n",
                "\n",
                "**Q4:** Looking at your Task 3 results, what was the compression ratio of Parquet vs CSV?\n",
                "\n",
                "---\n",
                "\n",
                "## âœ… Summary\n",
                "\n",
                "Today you learned:\n",
                "- [x] HDFS architecture: NameNode and DataNodes\n",
                "- [x] Data replication for fault tolerance\n",
                "- [x] File format comparison: CSV vs JSON vs Parquet\n",
                "- [x] Basic data loading with pandas\n",
                "- [x] HDFS command reference\n",
                "\n",
                "**Next Week:** MapReduce Programming"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cleanup: Remove temporary files\n",
                "import os\n",
                "for f in ['sample_data.csv', 'sample_data.json', 'sample_data.parquet']:\n",
                "    if os.path.exists(f):\n",
                "        os.remove(f)\n",
                "print(\"âœ… Temporary files cleaned up!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}