{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b98d0a03",
   "metadata": {},
   "source": [
    "# SE446 â€“ Week 2B: Hands-On HDFS Exploration\n",
    "\n",
    "## Exploring the Hadoop Distributed File System on a Real Cluster\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "1. **Understand HDFS Architecture**: Master/Worker topology with NameNode and DataNodes\n",
    "2. **Connect to a Production Cluster**: Access our live HDFS cluster securely\n",
    "3. **Execute HDFS Commands**: Create directories, upload/download files, manage data\n",
    "4. **Understand Replication**: See how data is replicated across DataNodes\n",
    "5. **Explore Block Distribution**: Understand how large files are split and distributed\n",
    "6. **Prepare for MapReduce**: Understand how HDFS enables distributed processing\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“‹ Prerequisites\n",
    "\n",
    "- Basic Python knowledge\n",
    "- Understanding of file systems\n",
    "- Completed Week 2A notebook (Big Data concepts)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ–¥ï¸ Our Cluster\n",
    "\n",
    "You have access to a **production HDFS cluster** deployed on DigitalOcean:\n",
    "\n",
    "| Component | Details |\n",
    "|-----------|----------|\n",
    "| **Hadoop Version** | 3.4.1 |\n",
    "| **Total Nodes** | 3 (1 Master + 2 Workers) |\n",
    "| **Total Storage** | 95.66 GB |\n",
    "| **Replication Factor** | 2 |\n",
    "| **Web UI** | https://hdfs.aniskoubaa.org |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f423340",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Environment Setup\n",
    "\n",
    "### 1.1 Install Required Packages\n",
    "\n",
    "We'll use `paramiko` for SSH connections and `python-dotenv` for secure credential management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c15d4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install paramiko python-dotenv requests pandas tabulate -q\n",
    "\n",
    "print(\"âœ… Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516b39bb",
   "metadata": {},
   "source": [
    "### 1.2 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1129497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Libraries imported successfully!\n",
      "ğŸ“… Session started: 2026-01-27 18:50:52\n"
     ]
    }
   ],
   "source": [
    "import paramiko\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ“š Libraries imported successfully!\")\n",
    "print(f\"ğŸ“… Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04180274",
   "metadata": {},
   "source": [
    "### 1.3 Configure Credentials Securely\n",
    "\n",
    "**âš ï¸ Security Best Practice**: Never hardcode passwords in notebooks!\n",
    "\n",
    "We'll use environment variables loaded from a `.env` file.\n",
    "\n",
    "#### Option A: Local Development\n",
    "Create a `.env` file in the same directory with:\n",
    "```\n",
    "HDFS_MASTER_HOST=134.209.172.50\n",
    "HDFS_SSH_USER=root\n",
    "HDFS_SSH_PASSWORD=your_password_here\n",
    "HDFS_HADOOP_USER=hadoop\n",
    "```\n",
    "\n",
    "#### Option B: Google Colab\n",
    "Use Colab's Secrets feature (ğŸ”‘ icon in the sidebar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "098fd83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’» Running locally\n",
      "ğŸ“‚ Found .env file at: /Applications/XAMPP/xamppfiles/htdocs/aniskoubaa.org/se446/big_data_course/lectures/week02/notebooks/.env\n",
      "âœ… Credentials loaded from .env file\n",
      "   Host: 134.209.172.50\n",
      "   SSH User: root\n",
      "   Password: ********\n",
      "   Hadoop User: hadoop\n"
     ]
    }
   ],
   "source": [
    "# Detect environment and load credentials\n",
    "def load_credentials():\n",
    "    \"\"\"Load HDFS credentials from .env file or Colab secrets.\"\"\"\n",
    "    \n",
    "    # Check if running in Google Colab\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        IN_COLAB = True\n",
    "        print(\"ğŸŒ Running in Google Colab\")\n",
    "    except ImportError:\n",
    "        IN_COLAB = False\n",
    "        print(\"ğŸ’» Running locally\")\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        # Load from Colab secrets\n",
    "        try:\n",
    "            credentials = {\n",
    "                'host': userdata.get('HDFS_MASTER_HOST'),\n",
    "                'ssh_user': userdata.get('HDFS_SSH_USER'),\n",
    "                'ssh_password': userdata.get('HDFS_SSH_PASSWORD'),\n",
    "                'hadoop_user': userdata.get('HDFS_HADOOP_USER')\n",
    "            }\n",
    "            print(\"âœ… Credentials loaded from Colab Secrets\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading Colab secrets: {e}\")\n",
    "            print(\"Please add secrets using the ğŸ”‘ icon in the sidebar\")\n",
    "            return None\n",
    "    else:\n",
    "        # Load from .env file\n",
    "        from dotenv import load_dotenv\n",
    "        \n",
    "        env_path = Path('.env')\n",
    "        if env_path.exists():\n",
    "            print(f\"ğŸ“‚ Found .env file at: {env_path.absolute()}\")\n",
    "            load_dotenv(env_path)\n",
    "            credentials = {\n",
    "                'host': os.getenv('HDFS_MASTER_HOST'),\n",
    "                'ssh_user': os.getenv('HDFS_SSH_USER'),\n",
    "                'ssh_password': os.getenv('HDFS_SSH_PASSWORD'),\n",
    "                'hadoop_user': os.getenv('HDFS_HADOOP_USER', 'hadoop')\n",
    "            }\n",
    "            \n",
    "            # Validate credentials\n",
    "            missing = []\n",
    "            for key, value in credentials.items():\n",
    "                if key != 'hadoop_user' and (not value or value.strip() == ''):\n",
    "                    missing.append(key)\n",
    "            \n",
    "            if missing:\n",
    "                print(f\"âŒ Missing or empty credentials: {', '.join(missing)}\")\n",
    "                print(\"\\nğŸ“‹ Current values:\")\n",
    "                for key, value in credentials.items():\n",
    "                    if key == 'ssh_password':\n",
    "                        print(f\"   {key}: {'*' * len(value) if value else 'NOT SET'}\")\n",
    "                    else:\n",
    "                        print(f\"   {key}: {value if value else 'NOT SET'}\")\n",
    "                print(\"\\nğŸ’¡ Check your .env file format. It should look like:\")\n",
    "                print(\"\"\"\n",
    "HDFS_MASTER_HOST=134.209.172.50\n",
    "HDFS_SSH_USER=root\n",
    "HDFS_SSH_PASSWORD=your_actual_password\n",
    "HDFS_HADOOP_USER=hadoop\n",
    "                \"\"\")\n",
    "                print(\"\\nâš ï¸ Common issues:\")\n",
    "                print(\"   â€¢ No spaces around = sign\")\n",
    "                print(\"   â€¢ No quotes around values\")\n",
    "                print(\"   â€¢ No comments on the same line\")\n",
    "                return None\n",
    "            \n",
    "            print(\"âœ… Credentials loaded from .env file\")\n",
    "            print(f\"   Host: {credentials['host']}\")\n",
    "            print(f\"   SSH User: {credentials['ssh_user']}\")\n",
    "            print(f\"   Password: {'*' * 8}\")\n",
    "            print(f\"   Hadoop User: {credentials['hadoop_user']}\")\n",
    "        else:\n",
    "            print(\"âŒ No .env file found!\")\n",
    "            print(f\"   Expected location: {env_path.absolute()}\")\n",
    "            print(\"\\nPlease create a .env file with the following content:\")\n",
    "            print(\"\"\"\n",
    "HDFS_MASTER_HOST=134.209.172.50\n",
    "HDFS_SSH_USER=root\n",
    "HDFS_SSH_PASSWORD=your_password\n",
    "HDFS_HADOOP_USER=hadoop\n",
    "            \"\"\")\n",
    "            return None\n",
    "    \n",
    "    return credentials\n",
    "\n",
    "# Load credentials\n",
    "CREDENTIALS = load_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac810aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Understanding HDFS Architecture\n",
    "\n",
    "Before connecting, let's understand what we're working with.\n",
    "\n",
    "### 2.1 Cluster Topology Overview\n",
    "\n",
    "```\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚           STUDENTS                  â”‚\n",
    "                    â”‚      (HTTPS via Browser)            â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                     â”‚\n",
    "                                     â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚      NGINX REVERSE PROXY            â”‚\n",
    "                    â”‚   SSL/TLS + Authentication          â”‚\n",
    "                    â”‚      hdfs.aniskoubaa.org            â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                     â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                           â”‚                           â”‚\n",
    "         â–¼                           â–¼                           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   MASTER NODE   â”‚       â”‚  WORKER NODE 1  â”‚       â”‚  WORKER NODE 2  â”‚\n",
    "â”‚ 134.209.172.50  â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚ 104.131.191.109 â”‚â—„â”€â”€â”€â”€â”€â–ºâ”‚ 157.245.211.168 â”‚\n",
    "â”‚                 â”‚       â”‚                 â”‚       â”‚                 â”‚\n",
    "â”‚   â€¢ NameNode    â”‚       â”‚   â€¢ DataNode    â”‚       â”‚   â€¢ DataNode    â”‚\n",
    "â”‚   â€¢ Secondary   â”‚       â”‚   â€¢ 48.28 GB    â”‚       â”‚   â€¢ 47.39 GB    â”‚\n",
    "â”‚     NameNode    â”‚       â”‚                 â”‚       â”‚                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                           â”‚                           â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           Heartbeats & Block Reports\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b40a0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our cluster configuration\n",
    "CLUSTER_CONFIG = {\n",
    "    'master': {\n",
    "        'hostname': 'master-node',\n",
    "        'ip': '134.209.172.50',\n",
    "        'role': 'Master',\n",
    "        'components': ['NameNode', 'SecondaryNameNode'],\n",
    "        'ports': {\n",
    "            'namenode_rpc': 9000,\n",
    "            'namenode_http': 9870,\n",
    "            'secondary_http': 9868\n",
    "        }\n",
    "    },\n",
    "    'workers': [\n",
    "        {\n",
    "            'hostname': 'worker-node-1',\n",
    "            'ip': '104.131.191.109',\n",
    "            'role': 'Worker',\n",
    "            'components': ['DataNode'],\n",
    "            'capacity_gb': 48.28\n",
    "        },\n",
    "        {\n",
    "            'hostname': 'worker-node-2',\n",
    "            'ip': '157.245.211.168',\n",
    "            'role': 'Worker',\n",
    "            'components': ['DataNode'],\n",
    "            'capacity_gb': 47.39\n",
    "        }\n",
    "    ],\n",
    "    'hdfs': {\n",
    "        'replication_factor': 2,\n",
    "        'block_size_mb': 128,\n",
    "        'total_capacity_gb': 95.67\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76d31e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ–¥ï¸  HDFS CLUSTER CONFIGURATION\n",
      "==================================================\n",
      "\n",
      "ğŸ“Š Master Node: master-node\n",
      "   IP: 134.209.172.50\n",
      "   Components: NameNode, SecondaryNameNode\n",
      "\n",
      "ğŸ“Š Worker Nodes:\n",
      "   1. worker-node-1 (104.131.191.109)\n",
      "      Capacity: 48.28 GB\n",
      "   2. worker-node-2 (157.245.211.168)\n",
      "      Capacity: 47.39 GB\n",
      "\n",
      "ğŸ“Š HDFS Settings:\n",
      "   Replication Factor: 2\n",
      "   Block Size: 128 MB\n",
      "   Total Capacity: 95.67 GB\n",
      "   Effective Capacity: 47.84 GB (due to replication)\n"
     ]
    }
   ],
   "source": [
    "# Display cluster info\n",
    "print(\"ğŸ–¥ï¸  HDFS CLUSTER CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nğŸ“Š Master Node: {CLUSTER_CONFIG['master']['hostname']}\")\n",
    "print(f\"   IP: {CLUSTER_CONFIG['master']['ip']}\")\n",
    "print(f\"   Components: {', '.join(CLUSTER_CONFIG['master']['components'])}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Worker Nodes:\")\n",
    "for i, worker in enumerate(CLUSTER_CONFIG['workers'], 1):\n",
    "    print(f\"   {i}. {worker['hostname']} ({worker['ip']})\")\n",
    "    print(f\"      Capacity: {worker['capacity_gb']} GB\")\n",
    "\n",
    "print(f\"\\nğŸ“Š HDFS Settings:\")\n",
    "print(f\"   Replication Factor: {CLUSTER_CONFIG['hdfs']['replication_factor']}\")\n",
    "print(f\"   Block Size: {CLUSTER_CONFIG['hdfs']['block_size_mb']} MB\")\n",
    "print(f\"   Total Capacity: {CLUSTER_CONFIG['hdfs']['total_capacity_gb']} GB\")\n",
    "print(f\"   Effective Capacity: {CLUSTER_CONFIG['hdfs']['total_capacity_gb']/2:.2f} GB (due to replication)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2305b17",
   "metadata": {},
   "source": [
    "### 2.2 Key HDFS Concepts\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **NameNode** | Master server that manages file system namespace and regulates access |\n",
    "| **DataNode** | Worker servers that store actual data blocks |\n",
    "| **Block** | Fixed-size chunk of data (default 128 MB) |\n",
    "| **Replication** | Multiple copies of each block for fault tolerance |\n",
    "| **Heartbeat** | Periodic signal from DataNode to NameNode (every 3 seconds) |\n",
    "| **Block Report** | List of all blocks on a DataNode, sent periodically |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a853d334",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Connecting to the HDFS Cluster\n",
    "\n",
    "### 3.1 Create SSH Connection Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "821868ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HDFSClusterConnection class defined\n"
     ]
    }
   ],
   "source": [
    "class HDFSClusterConnection:\n",
    "    \"\"\"\n",
    "    A helper class to connect to our HDFS cluster via SSH\n",
    "    and execute HDFS commands.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, credentials):\n",
    "        self.host = credentials['host']\n",
    "        self.ssh_user = credentials['ssh_user']\n",
    "        self.ssh_password = credentials['ssh_password']\n",
    "        self.hadoop_user = credentials.get('hadoop_user', 'hadoop')\n",
    "        self.client = None\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish SSH connection to the master node.\"\"\"\n",
    "        try:\n",
    "            self.client = paramiko.SSHClient()\n",
    "            self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "            self.client.connect(\n",
    "                hostname=self.host,\n",
    "                username=self.ssh_user,\n",
    "                password=self.ssh_password,\n",
    "                timeout=30\n",
    "            )\n",
    "            print(f\"âœ… Connected to master node at {self.host}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Connection failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def disconnect(self):\n",
    "        \"\"\"Close SSH connection.\"\"\"\n",
    "        if self.client:\n",
    "            self.client.close()\n",
    "            print(\"ğŸ”Œ Disconnected from cluster\")\n",
    "    \n",
    "    def execute_hdfs_command(self, command, show_output=True):\n",
    "        \"\"\"\n",
    "        Execute an HDFS command as the hadoop user.\n",
    "        \n",
    "        Args:\n",
    "            command: HDFS command (without 'hdfs dfs' prefix)\n",
    "            show_output: Whether to print the output\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (stdout, stderr, exit_code)\n",
    "        \"\"\"\n",
    "        if not self.client:\n",
    "            print(\"âŒ Not connected. Call connect() first.\")\n",
    "            return None, None, -1\n",
    "        \n",
    "        # Build the full command\n",
    "        full_command = f\"sudo -u {self.hadoop_user} hdfs dfs {command}\"\n",
    "        \n",
    "        if show_output:\n",
    "            print(f\"\\nğŸ”§ Executing: hdfs dfs {command}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        stdin, stdout, stderr = self.client.exec_command(full_command)\n",
    "        \n",
    "        out = stdout.read().decode('utf-8')\n",
    "        err = stderr.read().decode('utf-8')\n",
    "        exit_code = stdout.channel.recv_exit_status()\n",
    "        \n",
    "        if show_output:\n",
    "            if out:\n",
    "                print(out)\n",
    "            if err and exit_code != 0:\n",
    "                print(f\"âš ï¸ Error: {err}\")\n",
    "        \n",
    "        return out, err, exit_code\n",
    "    \n",
    "    def execute_admin_command(self, command, show_output=True):\n",
    "        \"\"\"\n",
    "        Execute an HDFS admin command.\n",
    "        \n",
    "        Args:\n",
    "            command: Admin command (without 'hdfs dfsadmin' prefix)\n",
    "        \"\"\"\n",
    "        if not self.client:\n",
    "            print(\"âŒ Not connected. Call connect() first.\")\n",
    "            return None, None, -1\n",
    "        \n",
    "        full_command = f\"sudo -u {self.hadoop_user} hdfs dfsadmin {command}\"\n",
    "        \n",
    "        if show_output:\n",
    "            print(f\"\\nğŸ”§ Executing: hdfs dfsadmin {command}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        stdin, stdout, stderr = self.client.exec_command(full_command)\n",
    "        \n",
    "        out = stdout.read().decode('utf-8')\n",
    "        err = stderr.read().decode('utf-8')\n",
    "        exit_code = stdout.channel.recv_exit_status()\n",
    "        \n",
    "        if show_output:\n",
    "            if out:\n",
    "                print(out)\n",
    "            if err and exit_code != 0:\n",
    "                print(f\"âš ï¸ Error: {err}\")\n",
    "        \n",
    "        return out, err, exit_code\n",
    "    \n",
    "    def execute_shell_command(self, command, show_output=True):\n",
    "        \"\"\"Execute a general shell command.\"\"\"\n",
    "        if not self.client:\n",
    "            print(\"âŒ Not connected. Call connect() first.\")\n",
    "            return None, None, -1\n",
    "        \n",
    "        if show_output:\n",
    "            print(f\"\\nğŸ”§ Executing: {command}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        stdin, stdout, stderr = self.client.exec_command(command)\n",
    "        \n",
    "        out = stdout.read().decode('utf-8')\n",
    "        err = stderr.read().decode('utf-8')\n",
    "        exit_code = stdout.channel.recv_exit_status()\n",
    "        \n",
    "        if show_output:\n",
    "            if out:\n",
    "                print(out)\n",
    "            if err and exit_code != 0:\n",
    "                print(f\"âš ï¸ Error: {err}\")\n",
    "        \n",
    "        return out, err, exit_code\n",
    "\n",
    "print(\"âœ… HDFSClusterConnection class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443b484",
   "metadata": {},
   "source": [
    "### 3.2 Establish Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d8ddd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to master node at 134.209.172.50\n",
      "\n",
      "ğŸ‰ You are now connected to the HDFS cluster!\n",
      "ğŸ“ Master Node: master-node (134.209.172.50)\n"
     ]
    }
   ],
   "source": [
    "# Create connection instance\n",
    "if CREDENTIALS:\n",
    "    hdfs = HDFSClusterConnection(CREDENTIALS)\n",
    "    \n",
    "    # Connect to the cluster\n",
    "    if hdfs.connect():\n",
    "        print(\"\\nğŸ‰ You are now connected to the HDFS cluster!\")\n",
    "        print(\"ğŸ“ Master Node: master-node (134.209.172.50)\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ Please check your credentials and try again.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Credentials not loaded. Please configure .env file or Colab secrets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59769a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Exploring the NameNode (Master)\n",
    "\n",
    "The **NameNode** is the brain of HDFS. It:\n",
    "- Manages the file system namespace (directory structure)\n",
    "- Stores metadata (file names, permissions, block locations)\n",
    "- Does NOT store actual data\n",
    "\n",
    "### 4.1 Check Cluster Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b2b28a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š HDFS CLUSTER REPORT\n",
      "============================================================\n",
      "\n",
      "ğŸ”§ Executing: hdfs dfsadmin -report\n",
      "--------------------------------------------------\n",
      "âš ï¸ Error: sudo: hdfs: command not found\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('', 'sudo: hdfs: command not found\\n', 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cluster report from NameNode\n",
    "print(\"ğŸ“Š HDFS CLUSTER REPORT\")\n",
    "print(\"=\" * 60)\n",
    "hdfs.execute_admin_command(\"-report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89862b1",
   "metadata": {},
   "source": [
    "### ğŸ¯ Exercise 4.1: Understanding the Report\n",
    "\n",
    "Look at the output above and answer:\n",
    "\n",
    "1. How many **Live datanodes** are there?\n",
    "2. What is the **Configured Capacity**?\n",
    "3. What is the **DFS Used** percentage?\n",
    "4. What is the **Block Pool Used**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fcecd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answers here (uncomment and fill in):\n",
    "\n",
    "# answer_4_1_1 = \"___\"  # Number of live datanodes\n",
    "# answer_4_1_2 = \"___\"  # Configured capacity\n",
    "# answer_4_1_3 = \"___\"  # DFS Used percentage\n",
    "# answer_4_1_4 = \"___\"  # Block pool used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa2ad7a",
   "metadata": {},
   "source": [
    "### 4.2 Check NameNode Safe Mode Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a023e774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”’ SAFE MODE STATUS\n",
      "========================================\n",
      "\n",
      "ğŸ”§ Executing: hdfs dfsadmin -safemode get\n",
      "--------------------------------------------------\n",
      "âš ï¸ Error: sudo: hdfs: command not found\n",
      "\n",
      "\n",
      "ğŸ’¡ Safe Mode: When NameNode is in safe mode, it is read-only.\n",
      "   This happens during startup or maintenance.\n"
     ]
    }
   ],
   "source": [
    "# Check if NameNode is in Safe Mode\n",
    "print(\"ğŸ”’ SAFE MODE STATUS\")\n",
    "print(\"=\" * 40)\n",
    "hdfs.execute_admin_command(\"-safemode get\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Safe Mode: When NameNode is in safe mode, it is read-only.\")\n",
    "print(\"   This happens during startup or maintenance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2760e1",
   "metadata": {},
   "source": [
    "### 4.3 View NameNode Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f161e96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ KEY HDFS CONFIGURATION\n",
      "==================================================\n",
      "ğŸ“‹ Replication Factor Configuration:\n",
      "    <name>dfs.replication</name>\n",
      "    <value>2</value>\n",
      "\n",
      "ğŸ“‹ Block Size Configuration:\n",
      "Using default: 128 MB\n"
     ]
    }
   ],
   "source": [
    "# Check key HDFS configuration\n",
    "print(\"âš™ï¸ KEY HDFS CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get replication factor\n",
    "out, _, _ = hdfs.execute_shell_command(\n",
    "    \"grep -A1 'dfs.replication' /opt/hadoop/etc/hadoop/hdfs-site.xml\",\n",
    "    show_output=False\n",
    ")\n",
    "print(f\"ğŸ“‹ Replication Factor Configuration:\")\n",
    "print(out)\n",
    "\n",
    "# Get block size\n",
    "out, _, _ = hdfs.execute_shell_command(\n",
    "    \"grep -A1 'dfs.blocksize' /opt/hadoop/etc/hadoop/hdfs-site.xml\",\n",
    "    show_output=False\n",
    ")\n",
    "print(f\"ğŸ“‹ Block Size Configuration:\")\n",
    "print(out if out else \"Using default: 128 MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5d36f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Exploring DataNodes (Workers)\n",
    "\n",
    "**DataNodes** are the workhorses of HDFS. They:\n",
    "- Store actual data blocks\n",
    "- Send heartbeats to NameNode every 3 seconds\n",
    "- Report block information periodically\n",
    "- Perform read/write operations\n",
    "\n",
    "### 5.1 List All DataNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "528c73e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š DATANODE INFORMATION\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Get DataNode information\n",
    "print(\"ğŸ“Š DATANODE INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "out, _, _ = hdfs.execute_admin_command(\"-report\", show_output=False)\n",
    "\n",
    "# Parse DataNode information\n",
    "import re\n",
    "\n",
    "datanode_pattern = r'Name: (\\S+).*?Hostname: (\\S+).*?Configured Capacity: (\\d+).*?DFS Used: (\\d+).*?DFS Remaining: (\\d+)'\n",
    "datanodes = re.findall(datanode_pattern, out, re.DOTALL)\n",
    "\n",
    "if datanodes:\n",
    "    print(\"\\nğŸ“ Live DataNodes:\\n\")\n",
    "    for i, dn in enumerate(datanodes, 1):\n",
    "        name, hostname, capacity, used, remaining = dn\n",
    "        capacity_gb = int(capacity) / (1024**3)\n",
    "        used_gb = int(used) / (1024**3)\n",
    "        remaining_gb = int(remaining) / (1024**3)\n",
    "        \n",
    "        print(f\"  DataNode {i}: {hostname}\")\n",
    "        print(f\"    â”œâ”€ IP:Port: {name}\")\n",
    "        print(f\"    â”œâ”€ Capacity: {capacity_gb:.2f} GB\")\n",
    "        print(f\"    â”œâ”€ Used: {used_gb:.2f} GB ({(used_gb/capacity_gb)*100:.1f}%)\")\n",
    "        print(f\"    â””â”€ Remaining: {remaining_gb:.2f} GB\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66219394",
   "metadata": {},
   "source": [
    "### 5.2 Understanding Heartbeats\n",
    "\n",
    "DataNodes send **heartbeats** to the NameNode every 3 seconds to indicate they are alive.\n",
    "\n",
    "If the NameNode doesn't receive a heartbeat for **10 minutes** (default), it marks the DataNode as **dead**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4ffe0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’“ DATANODE HEALTH CHECK\n",
      "==================================================\n",
      "\n",
      "âš ï¸ Some DataNodes may be offline\n",
      "   â””â”€ Check the cluster report for details\n",
      "\n",
      "ğŸ“ Heartbeat Configuration:\n",
      "   â”œâ”€ Heartbeat Interval: 3 seconds\n",
      "   â””â”€ Dead Node Timeout: 10 minutes\n"
     ]
    }
   ],
   "source": [
    "# Check for dead nodes\n",
    "print(\"ğŸ’“ DATANODE HEALTH CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Look for \"Dead datanodes\" in the report\n",
    "if \"Dead datanodes (0)\" in out or \"Dead datanodes: 0\" in out.replace('(', ':').replace(')', ''):\n",
    "    print(\"\\nâœ… All DataNodes are healthy!\")\n",
    "    print(\"   â””â”€ Heartbeats are being received from all nodes\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Some DataNodes may be offline\")\n",
    "    print(\"   â””â”€ Check the cluster report for details\")\n",
    "\n",
    "print(\"\\nğŸ“ Heartbeat Configuration:\")\n",
    "print(\"   â”œâ”€ Heartbeat Interval: 3 seconds\")\n",
    "print(\"   â””â”€ Dead Node Timeout: 10 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76b991",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Basic HDFS Commands\n",
    "\n",
    "HDFS commands are similar to Unix file system commands!\n",
    "\n",
    "| HDFS Command | Unix Equivalent | Description |\n",
    "|--------------|-----------------|-------------|\n",
    "| `hdfs dfs -ls` | `ls` | List files |\n",
    "| `hdfs dfs -mkdir` | `mkdir` | Create directory |\n",
    "| `hdfs dfs -put` | `cp` (to HDFS) | Upload file |\n",
    "| `hdfs dfs -get` | `cp` (from HDFS) | Download file |\n",
    "| `hdfs dfs -cat` | `cat` | View file content |\n",
    "| `hdfs dfs -rm` | `rm` | Delete file |\n",
    "| `hdfs dfs -du` | `du` | Disk usage |\n",
    "\n",
    "### 6.1 List Root Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f7d458f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ HDFS ROOT DIRECTORY\n",
      "==================================================\n",
      "\n",
      "ğŸ”§ Executing: hdfs dfs -ls /\n",
      "--------------------------------------------------\n",
      "âš ï¸ Error: sudo: hdfs: command not found\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('', 'sudo: hdfs: command not found\\n', 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the root directory\n",
    "print(\"ğŸ“ HDFS ROOT DIRECTORY\")\n",
    "print(\"=\" * 50)\n",
    "hdfs.execute_hdfs_command(\"-ls /\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b41cd09",
   "metadata": {},
   "source": [
    "### 6.2 Create Your Personal Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc7cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for your experiments\n",
    "# Replace 'student_demo' with your actual name/ID\n",
    "STUDENT_NAME = \"student_demo\"  # â† CHANGE THIS!\n",
    "\n",
    "student_dir = f\"/students/{STUDENT_NAME}\"\n",
    "\n",
    "print(f\"ğŸ“ Creating directory: {student_dir}\")\n",
    "hdfs.execute_hdfs_command(f\"-mkdir -p {student_dir}\")\n",
    "\n",
    "# Verify creation\n",
    "print(f\"\\nğŸ“‹ Verifying directory exists:\")\n",
    "hdfs.execute_hdfs_command(\"-ls /students\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d15d4aa",
   "metadata": {},
   "source": [
    "### 6.3 Upload a File to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255e88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a sample file locally on the master node\n",
    "sample_data = \"\"\"id,name,department,salary\n",
    "1,Alice,Engineering,75000\n",
    "2,Bob,Marketing,65000\n",
    "3,Charlie,Engineering,80000\n",
    "4,Diana,HR,60000\n",
    "5,Eve,Marketing,70000\n",
    "\"\"\"\n",
    "\n",
    "# Create the file on the remote server\n",
    "create_file_cmd = f\"echo '{sample_data}' > /tmp/employees.csv\"\n",
    "hdfs.execute_shell_command(create_file_cmd, show_output=False)\n",
    "print(\"âœ… Created sample file: /tmp/employees.csv\")\n",
    "\n",
    "# Upload to HDFS\n",
    "print(f\"\\nğŸ“¤ Uploading file to HDFS...\")\n",
    "hdfs.execute_hdfs_command(f\"-put /tmp/employees.csv {student_dir}/employees.csv\")\n",
    "\n",
    "# Verify upload\n",
    "print(f\"\\nğŸ“‹ Files in {student_dir}:\")\n",
    "hdfs.execute_hdfs_command(f\"-ls {student_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424b7bf3",
   "metadata": {},
   "source": [
    "### 6.4 View File Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372a58b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View file content using -cat\n",
    "print(\"ğŸ“„ FILE CONTENTS\")\n",
    "print(\"=\" * 40)\n",
    "hdfs.execute_hdfs_command(f\"-cat {student_dir}/employees.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f39346",
   "metadata": {},
   "source": [
    "### 6.5 Check File Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed file information\n",
    "print(\"ğŸ“Š FILE STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# File size\n",
    "print(\"\\nğŸ“ File Size:\")\n",
    "hdfs.execute_hdfs_command(f\"-du -h {student_dir}/employees.csv\")\n",
    "\n",
    "# Detailed stats\n",
    "print(\"\\nğŸ“‹ Detailed Statistics:\")\n",
    "hdfs.execute_hdfs_command(f\"-stat 'Replication: %r, Block Size: %o, Size: %b bytes' {student_dir}/employees.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3fcc6b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Understanding Replication\n",
    "\n",
    "**Replication** is HDFS's primary mechanism for fault tolerance.\n",
    "\n",
    "### 7.1 How Replication Works\n",
    "\n",
    "```\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚    NameNode     â”‚\n",
    "                    â”‚                 â”‚\n",
    "                    â”‚  Block Map:     â”‚\n",
    "                    â”‚  Block1 â†’ DN1,DN2â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                   â”‚                   â”‚\n",
    "         â–¼                   â–¼                   â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   DataNode 1    â”‚ â”‚   DataNode 2    â”‚ â”‚   DataNode 3    â”‚\n",
    "â”‚                 â”‚ â”‚                 â”‚ â”‚                 â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚                 â”‚\n",
    "â”‚   â”‚ Block 1 â”‚   â”‚ â”‚   â”‚ Block 1 â”‚   â”‚ â”‚   (no replica)  â”‚\n",
    "â”‚   â”‚ (copy)  â”‚   â”‚ â”‚   â”‚ (copy)  â”‚   â”‚ â”‚                 â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "With Replication Factor = 2, each block is stored on 2 different DataNodes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1032650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check replication of our file\n",
    "print(\"ğŸ”„ REPLICATION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get file info including block locations\n",
    "out, _, _ = hdfs.execute_shell_command(\n",
    "    f\"sudo -u hadoop hdfs fsck {student_dir}/employees.csv -files -blocks -locations\",\n",
    "    show_output=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90425b7c",
   "metadata": {},
   "source": [
    "### 7.2 Change Replication Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712767b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current replication\n",
    "print(\"ğŸ“Š Current Replication Factor:\")\n",
    "hdfs.execute_hdfs_command(f\"-stat '%r' {student_dir}/employees.csv\")\n",
    "\n",
    "# Note: We have only 2 DataNodes, so max replication is 2\n",
    "print(\"\\nğŸ’¡ Our cluster has 2 DataNodes, so maximum replication = 2\")\n",
    "print(\"   If you try to set replication > 2, HDFS will only use 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e526c2",
   "metadata": {},
   "source": [
    "### ğŸ¯ Exercise 7.1: Understanding Replication Impact\n",
    "\n",
    "If you upload a **100 MB** file with **replication factor = 2**:\n",
    "\n",
    "1. How much **actual storage** is used on the cluster?\n",
    "2. How many **copies** of each block exist?\n",
    "3. What happens if **one DataNode fails**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef429e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answers here:\n",
    "\n",
    "# answer_7_1_1 = \"___\"  # Actual storage used\n",
    "# answer_7_1_2 = \"___\"  # Number of copies\n",
    "# answer_7_1_3 = \"___\"  # What happens on failure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526759a6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Understanding Block Distribution\n",
    "\n",
    "Large files are split into **blocks** (default 128 MB) and distributed across DataNodes.\n",
    "\n",
    "### 8.1 How Block Distribution Works\n",
    "\n",
    "```\n",
    "Original File: 300 MB\n",
    "Block Size: 128 MB\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     300 MB File                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼ Split into blocks\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Block 1     â”‚ â”‚   Block 2     â”‚ â”‚   Block 3     â”‚\n",
    "â”‚   128 MB      â”‚ â”‚   128 MB      â”‚ â”‚    44 MB      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â”‚                 â”‚                 â”‚\n",
    "        â–¼                 â–¼                 â–¼ Distribute across nodes\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  DataNode 1   â”‚ â”‚  DataNode 2   â”‚ â”‚  DataNode 1   â”‚\n",
    "â”‚   Block 1     â”‚ â”‚   Block 2     â”‚ â”‚   Block 3     â”‚\n",
    "â”‚   Block 2     â”‚ â”‚   Block 1     â”‚ â”‚   Block 2     â”‚\n",
    "â”‚   (replica)   â”‚ â”‚   (replica)   â”‚ â”‚   (replica)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf004b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger file to demonstrate block distribution\n",
    "print(\"ğŸ“¦ CREATING LARGER FILE FOR BLOCK DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate a 5MB file (enough to see distribution, small enough to be fast)\n",
    "hdfs.execute_shell_command(\n",
    "    \"dd if=/dev/urandom of=/tmp/sample_5mb.bin bs=1M count=5 2>/dev/null\",\n",
    "    show_output=False\n",
    ")\n",
    "print(\"âœ… Created 5 MB binary file\")\n",
    "\n",
    "# Upload to HDFS\n",
    "print(\"\\nğŸ“¤ Uploading to HDFS...\")\n",
    "hdfs.execute_hdfs_command(f\"-put /tmp/sample_5mb.bin {student_dir}/sample_5mb.bin\")\n",
    "\n",
    "# Check block distribution\n",
    "print(\"\\nğŸ“Š Block Distribution Analysis:\")\n",
    "hdfs.execute_shell_command(\n",
    "    f\"sudo -u hadoop hdfs fsck {student_dir}/sample_5mb.bin -files -blocks -locations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f633208",
   "metadata": {},
   "source": [
    "### 8.2 Visualize Storage Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96afa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check storage usage on each DataNode\n",
    "print(\"ğŸ“Š STORAGE DISTRIBUTION ACROSS DATANODES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "out, _, _ = hdfs.execute_admin_command(\"-report\", show_output=False)\n",
    "\n",
    "# Parse and display storage info\n",
    "lines = out.split('\\n')\n",
    "current_node = None\n",
    "storage_data = []\n",
    "\n",
    "for line in lines:\n",
    "    if 'Hostname:' in line:\n",
    "        current_node = line.split(':')[1].strip()\n",
    "    elif 'DFS Used:' in line and current_node:\n",
    "        try:\n",
    "            used = int(line.split(':')[1].strip().split()[0])\n",
    "            used_gb = used / (1024**3)\n",
    "            storage_data.append({'Node': current_node, 'Used (GB)': f\"{used_gb:.4f}\"})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if storage_data:\n",
    "    df = pd.DataFrame(storage_data)\n",
    "    print(df.to_string(index=False))\n",
    "    print(\"\\nğŸ’¡ Notice how data is distributed across both DataNodes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc95db6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Reading Files from Distributed Storage\n",
    "\n",
    "When you read a file from HDFS, the client:\n",
    "1. Asks NameNode for block locations\n",
    "2. Reads blocks directly from DataNodes\n",
    "3. Assembles blocks into the complete file\n",
    "\n",
    "### 9.1 The Read Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff3bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate reading a file\n",
    "print(\"ğŸ“– HDFS READ OPERATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nğŸ“‹ Step 1: Get block locations from NameNode\")\n",
    "print(\"-\" * 40)\n",
    "hdfs.execute_shell_command(\n",
    "    f\"sudo -u hadoop hdfs fsck {student_dir}/employees.csv -files -blocks -locations 2>/dev/null | grep -E 'blk_|DatanodeInfoWithStorage'\",\n",
    "    show_output=True\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“‹ Step 2: Read file content (assembled from blocks)\")\n",
    "print(\"-\" * 40)\n",
    "hdfs.execute_hdfs_command(f\"-cat {student_dir}/employees.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab491f74",
   "metadata": {},
   "source": [
    "### 9.2 Download File from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0297842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download file from HDFS to local\n",
    "print(\"ğŸ“¥ DOWNLOADING FILE FROM HDFS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Download using -get\n",
    "hdfs.execute_hdfs_command(f\"-get {student_dir}/employees.csv /tmp/downloaded_employees.csv\")\n",
    "\n",
    "# Verify download\n",
    "print(\"\\nâœ… File downloaded! Verifying content:\")\n",
    "hdfs.execute_shell_command(\"cat /tmp/downloaded_employees.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ac1e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Preparing for MapReduce\n",
    "\n",
    "Now that you understand HDFS, you're ready for **MapReduce**!\n",
    "\n",
    "### 10.1 Why HDFS + MapReduce Work Together\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        MapReduce Job                                 â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚   Input (HDFS)          Map Phase           Reduce Phase   Output   â”‚\n",
    "â”‚                                                            (HDFS)   â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚   â”‚ Block 1 â”‚  â”€â”€â”€â–º    â”‚ Map 1  â”‚  â”€â”€â”     â”‚        â”‚              â”‚\n",
    "â”‚   â”‚(Node 1) â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚     â”‚        â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”œâ”€â”€â”€â–º â”‚Reduce 1â”‚â”€â”€â–ºâ”‚Result â”‚  â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚     â”‚        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚   â”‚ Block 2 â”‚  â”€â”€â”€â–º    â”‚ Map 2  â”‚  â”€â”€â”˜     â”‚        â”‚              â”‚\n",
    "â”‚   â”‚(Node 2) â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\n",
    "â”‚                                                                      â”‚\n",
    "â”‚   Key Insight: Map tasks run WHERE the data is stored (data locality)â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Data Locality**: MapReduce moves computation to data, not data to computation\n",
    "- **Parallel Processing**: Each block can be processed by a separate Map task\n",
    "- **Fault Tolerance**: If a task fails, HDFS replication allows restart on another node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if MapReduce/YARN is available\n",
    "print(\"ğŸ” CHECKING MAPREDUCE READINESS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check Hadoop version\n",
    "print(\"\\nğŸ“‹ Hadoop Version:\")\n",
    "hdfs.execute_shell_command(\"hadoop version | head -3\")\n",
    "\n",
    "# Check HDFS status for MapReduce\n",
    "print(\"\\nğŸ“‹ HDFS Status (for MapReduce input):\")\n",
    "hdfs.execute_hdfs_command(f\"-count {student_dir}\")\n",
    "\n",
    "print(\"\\nâœ… Your HDFS cluster is ready for MapReduce!\")\n",
    "print(\"   â””â”€ Next lecture: We'll run MapReduce jobs on this data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7d76bf",
   "metadata": {},
   "source": [
    "### 10.2 Create Sample Data for MapReduce Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027773f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word count sample data (classic MapReduce example)\n",
    "print(\"ğŸ“ CREATING SAMPLE DATA FOR MAPREDUCE LAB\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "word_count_data = \"\"\"Hello World\n",
    "Hello Hadoop\n",
    "Hello HDFS\n",
    "Hadoop is great\n",
    "HDFS stores data\n",
    "MapReduce processes data\n",
    "Hello Big Data\n",
    "Big Data is the future\n",
    "Hadoop HDFS MapReduce\n",
    "Data Data Data\n",
    "\"\"\"\n",
    "\n",
    "# Create and upload\n",
    "hdfs.execute_shell_command(f\"echo '{word_count_data}' > /tmp/wordcount_input.txt\", show_output=False)\n",
    "hdfs.execute_hdfs_command(f\"-mkdir -p {student_dir}/mapreduce_lab\")\n",
    "hdfs.execute_hdfs_command(f\"-put /tmp/wordcount_input.txt {student_dir}/mapreduce_lab/input.txt\")\n",
    "\n",
    "print(\"\\nâœ… Sample data created for MapReduce lab!\")\n",
    "print(f\"\\nğŸ“‹ File location: {student_dir}/mapreduce_lab/input.txt\")\n",
    "print(\"\\nğŸ“„ Content:\")\n",
    "hdfs.execute_hdfs_command(f\"-cat {student_dir}/mapreduce_lab/input.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56915eb3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. HDFS Command Reference\n",
    "\n",
    "### Quick Reference Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7462f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print HDFS command reference\n",
    "print(\"ğŸ“š HDFS COMMAND QUICK REFERENCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "commands = [\n",
    "    [\"Category\", \"Command\", \"Description\"],\n",
    "    [\"â”€\" * 10, \"â”€\" * 30, \"â”€\" * 25],\n",
    "    [\"Navigation\", \"hdfs dfs -ls <path>\", \"List directory contents\"],\n",
    "    [\"\", \"hdfs dfs -ls -R <path>\", \"List recursively\"],\n",
    "    [\"\", \"hdfs dfs -ls -h <path>\", \"Human-readable sizes\"],\n",
    "    [\"â”€\" * 10, \"â”€\" * 30, \"â”€\" * 25],\n",
    "    [\"Directory\", \"hdfs dfs -mkdir <path>\", \"Create directory\"],\n",
    "    [\"\", \"hdfs dfs -mkdir -p <path>\", \"Create with parents\"],\n",
    "    [\"â”€\" * 10, \"â”€\" * 30, \"â”€\" * 25],\n",
    "    [\"File Ops\", \"hdfs dfs -put <src> <dst>\", \"Upload file\"],\n",
    "    [\"\", \"hdfs dfs -get <src> <dst>\", \"Download file\"],\n",
    "    [\"\", \"hdfs dfs -cat <file>\", \"View file content\"],\n",
    "    [\"\", \"hdfs dfs -tail <file>\", \"View last 1KB\"],\n",
    "    [\"\", \"hdfs dfs -head <file>\", \"View first 1KB\"],\n",
    "    [\"â”€\" * 10, \"â”€\" * 30, \"â”€\" * 25],\n",
    "    [\"Delete\", \"hdfs dfs -rm <file>\", \"Delete file\"],\n",
    "    [\"\", \"hdfs dfs -rm -r <dir>\", \"Delete directory\"],\n",
    "    [\"\", \"hdfs dfs -rm -skipTrash <f>\", \"Delete permanently\"],\n",
    "    [\"â”€\" * 10, \"â”€\" * 30, \"â”€\" * 25],\n",
    "    [\"Info\", \"hdfs dfs -du -h <path>\", \"Disk usage\"],\n",
    "    [\"\", \"hdfs dfs -df -h\", \"Free space\"],\n",
    "    [\"\", \"hdfs dfs -stat <file>\", \"File statistics\"],\n",
    "    [\"\", \"hdfs dfs -count <path>\", \"Count files/dirs/bytes\"],\n",
    "    [\"â”€\" * 10, \"â”€\" * 30, \"â”€\" * 25],\n",
    "    [\"Admin\", \"hdfs dfsadmin -report\", \"Cluster report\"],\n",
    "    [\"\", \"hdfs fsck <path>\", \"File system check\"],\n",
    "]\n",
    "\n",
    "for row in commands:\n",
    "    print(f\"{row[0]:<12} {row[1]:<32} {row[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d552de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Cleanup and Disconnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f07534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up test files\n",
    "print(\"ğŸ§¹ CLEANUP (Optional)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Uncomment to delete test files:\n",
    "# hdfs.execute_hdfs_command(f\"-rm -r {student_dir}\")\n",
    "# print(f\"âœ… Deleted {student_dir}\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Your files in HDFS:\")\n",
    "hdfs.execute_hdfs_command(f\"-ls -R {student_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disconnect from cluster\n",
    "hdfs.disconnect()\n",
    "print(\"\\nâœ… Session complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fa42e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "| Topic | Key Takeaway |\n",
    "|-------|---------------|\n",
    "| **HDFS Architecture** | Master (NameNode) + Workers (DataNodes) |\n",
    "| **NameNode** | Manages metadata, doesn't store data |\n",
    "| **DataNode** | Stores actual blocks, sends heartbeats |\n",
    "| **Blocks** | Files are split into 128 MB chunks |\n",
    "| **Replication** | Each block stored on multiple nodes |\n",
    "| **Commands** | Similar to Unix: ls, mkdir, put, get, cat |\n",
    "| **MapReduce Ready** | HDFS provides input for distributed processing |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Next Steps\n",
    "\n",
    "1. **Explore More**: Use the Web UI at https://hdfs.aniskoubaa.org\n",
    "2. **Practice**: Upload larger files and observe block distribution\n",
    "3. **Prepare**: Review MapReduce concepts for the next lecture\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Resources\n",
    "\n",
    "- [Apache Hadoop Documentation](https://hadoop.apache.org/docs/r3.4.1/)\n",
    "- [HDFS Architecture Guide](https://hadoop.apache.org/docs/r3.4.1/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)\n",
    "- Cluster Web UI: https://hdfs.aniskoubaa.org (Username: admin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
